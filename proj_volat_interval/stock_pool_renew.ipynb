{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import datetime\r\n",
    "from data_tools.api import *\r\n",
    "from utilscht.Data import *\r\n",
    "from joblib import Parallel,delayed\r\n",
    "import logging\r\n",
    "from reader.reader import BarReader\r\n",
    "\r\n",
    "logging.basicConfig(level=logging.DEBUG,#控制台打印的日志级别\r\n",
    "                    filename=r'logging/stock_pool_renew.log',\r\n",
    "                    filemode='a',##模式，有w和a，w就是写模式，每次都会重新写日志，覆盖之前的日志\r\n",
    "                    #a是追加模式，默认如果不写的话，就是追加模式\r\n",
    "                    format='%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s'#日志格式\r\n",
    "                   )\r\n",
    "\r\n",
    "def apply_parallel(df_grouped, func, n_jobs=16, backend='loky', as_index=True, **kwargs):\r\n",
    "    \"\"\"\r\n",
    "    This is for parallel between grouped generated by pd.DataFrame.groupby\r\n",
    "    :param df_grouped:\r\n",
    "    :param func:\r\n",
    "    :param n_jobs:\r\n",
    "    :param backend:\r\n",
    "    :param kwargs:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    names = []\r\n",
    "    groups = []\r\n",
    "    for name, group in df_grouped:\r\n",
    "        names.append(name)\r\n",
    "        groups.append(group)\r\n",
    "\r\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=5, backend=backend, batch_size='auto') \\\r\n",
    "        (delayed(func)(group, **kwargs) for group in groups)\r\n",
    "\r\n",
    "    return pd.concat(results, keys=names if as_index else None)\r\n",
    "\r\n",
    "from data_tools.api import trade_days\r\n",
    "\r\n",
    "trade_dates_all = trade_days.copy()\r\n",
    "\r\n",
    "def get_prev_n_trade_date(trade_date, n):\r\n",
    "    trade_date=str(trade_date)[0:10].replace('-','')\r\n",
    "    pos = np.searchsorted(trade_dates_all, trade_date)\r\n",
    "    try:\r\n",
    "        assert pos >= n\r\n",
    "        return str(trade_dates_all[pos - n])\r\n",
    "    except AssertionError:\r\n",
    "        return \"20201231\"\r\n",
    "\r\n",
    "def get_next_n_trade_date(trade_date, n=1):\r\n",
    "    trade_date=str(trade_date)[0:10].replace('-','')\r\n",
    "    pos = np.searchsorted(trade_dates_all, trade_date, side='right')\r\n",
    "    try:\r\n",
    "        assert pos + n - 1 < len(trade_dates_all)\r\n",
    "        return str(trade_dates_all[pos + n - 1])\r\n",
    "    except AssertionError:\r\n",
    "        return \"20201231\"\r\n",
    "    \r\n",
    "def get_date_stop_pl(df,ratio = 0.1):\r\n",
    "    sid = df[\"sid\"].iloc[0]\r\n",
    "    date = df[\"date\"].iloc[0]\r\n",
    "    date_now = str(datetime.datetime.now().date()).replace('-','')\r\n",
    "    \r\n",
    "    df=pd.DataFrame(index=[0],columns=[\"date_{}p_ud\".format(int(ratio*100)),\"up_down\"])\r\n",
    "    if date == date_now:\r\n",
    "        df.iloc[0]=(str(date)[0:10],0)\r\n",
    "        return df\r\n",
    "    \r\n",
    "    hq_data = get_stk_bar(sid,start=date,end=get_next_n_trade_date(date,21),fields=[\"adj_close\",'adj_high',\"adj_low\"])\r\n",
    "    thr_up= hq_data[\"adj_close\"].values[0]*(1+ratio)\r\n",
    "    thr_down= hq_data[\"adj_close\"].values[0]*(1-ratio)\r\n",
    "    \r\n",
    "    with BarReader('/home/ywang/proj_cross_grid/reader/bar.yaml') as client:\r\n",
    "        bars = client.stock_bars(sid[0:6])\r\n",
    "        hq_df = pd.DataFrame(bars)\r\n",
    "        low = np.nanmin(hq_df[\"low\"])\r\n",
    "        high =  np.nanmax(hq_df[\"high\"])\r\n",
    "        close = hq_df[\"close\"].dropna().iloc[-1]\r\n",
    "    adj_factor = get_stk_bar(sid,start = get_previous_trade_date(date_now),\r\n",
    "                             end = get_previous_trade_date(date_now),fields=[\"adj_factor\"]).values[0][0]\r\n",
    "    hq_data = pd.concat([hq_data,pd.DataFrame(index = [date_now],columns=[\"adj_close\",'adj_high',\"adj_low\"],\r\n",
    "                                             data =[(close*adj_factor,high*adj_factor,low*adj_factor)])])\r\n",
    "    print(hq_data)\r\n",
    "    for dt in hq_data.index[1:21]:\r\n",
    "        if hq_data.loc[dt,\"adj_high\"] > thr_up:\r\n",
    "            df.iloc[0]=[(str(dt)[0:10], 1)]\r\n",
    "            return df\r\n",
    "        if hq_data.loc[dt,\"adj_low\"] < thr_down:\r\n",
    "            df.iloc[0]=[(str(dt)[0:10],-1)]\r\n",
    "            return df\r\n",
    "    \r\n",
    "    change_rate = hq_data.loc[dt,\"adj_close\"]/hq_data[\"adj_close\"].values[0]-1\r\n",
    "    df.iloc[0]=(str(dt)[0:10],change_rate)\r\n",
    "    return df\r\n",
    "\r\n",
    "def date_filter(df):\r\n",
    "    use_dates=[]\r\n",
    "    date_list = df[\"date\"].values\r\n",
    "    while(1):\r\n",
    "        if len(date_list)==0:\r\n",
    "            break\r\n",
    "        else:\r\n",
    "            date =  min(date_list)\r\n",
    "            use_dates.append(date)\r\n",
    "            dates_rm = get_trade_dates(date,min(get_next_n_trade_date(date,19),\"20201231\"))\r\n",
    "            date_list=list(set(date_list)-set(dates_rm))\r\n",
    "    return df[df[\"date\"].isin(use_dates)]\r\n",
    "\r\n",
    "\r\n",
    "def get_today_stk_data(sid,date):\r\n",
    "    try:\r\n",
    "        with BarReader('/home/ywang/proj_cross_grid/reader/bar.yaml') as client:\r\n",
    "            bars = client.stock_bars(sid[0:6])\r\n",
    "            df = pd.DataFrame(bars)\r\n",
    "            df[\"sid\"] = df[\"symbol\"].apply(lambda x:str(x)[2:-1])\r\n",
    "            df[\"low\"] = np.nanmin(df[\"low\"])\r\n",
    "            df= df[df[\"sid\"] != \"\"]\r\n",
    "            return df[[\"sid\",\"low\"]].iloc[0:1]\r\n",
    "    except:\r\n",
    "        logging.warning(\"Data Lost for {} on {}\".format(sid,date))\r\n",
    "\r\n",
    "def get_today_data(date):\r\n",
    "    pre_date = get_previous_trade_date(date)\r\n",
    "    df=query_table(\"DailyBar\",start_date=pre_date,end_date=pre_date,fields=[\"tradable\"])\r\n",
    "    stk_ls=list(df[\"sid\"])\r\n",
    "\r\n",
    "    results = Parallel(n_jobs=16, verbose=5, backend=\"loky\", batch_size='auto') \\\r\n",
    "        (delayed(get_today_stk_data)(sid,date) for sid in stk_ls)\r\n",
    "    data = pd.concat(results)\r\n",
    "\r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#!/home/ywang/env/dev/bin\r\n",
    "#-*-coding:utf-8-*-\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import datetime\r\n",
    "from data_tools.api import *\r\n",
    "from utilscht.Data import *\r\n",
    "from joblib import Parallel,delayed\r\n",
    "import logging\r\n",
    "\r\n",
    "logging.basicConfig(level=logging.DEBUG,#控制台打印的日志级别\r\n",
    "                    filename=r'logging/stock_pool_renew.log',\r\n",
    "                    filemode='a',##模式，有w和a，w就是写模式，每次都会重新写日志，覆盖之前的日志\r\n",
    "                    #a是追加模式，默认如果不写的话，就是追加模式\r\n",
    "                    format='%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s'#日志格式\r\n",
    "                   )\r\n",
    "\r\n",
    "def apply_parallel(df_grouped, func, n_jobs=16, backend='loky', as_index=True, **kwargs):\r\n",
    "    \"\"\"\r\n",
    "    This is for parallel between grouped generated by pd.DataFrame.groupby\r\n",
    "    :param df_grouped:\r\n",
    "    :param func:\r\n",
    "    :param n_jobs:\r\n",
    "    :param backend:\r\n",
    "    :param kwargs:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    names = []\r\n",
    "    groups = []\r\n",
    "    for name, group in df_grouped:\r\n",
    "        names.append(name)\r\n",
    "        groups.append(group)\r\n",
    "\r\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=5, backend=backend, batch_size='auto') \\\r\n",
    "        (delayed(func)(group, **kwargs) for group in groups)\r\n",
    "\r\n",
    "    return pd.concat(results, keys=names if as_index else None)\r\n",
    "\r\n",
    "from data_tools.api import trade_days\r\n",
    "\r\n",
    "trade_dates_all = trade_days.copy()\r\n",
    "\r\n",
    "def get_prev_n_trade_date(trade_date, n):\r\n",
    "    trade_date=str(trade_date)[0:10].replace('-','')\r\n",
    "    pos = np.searchsorted(trade_dates_all, trade_date)\r\n",
    "    try:\r\n",
    "        assert pos >= n\r\n",
    "        return str(trade_dates_all[pos - n])\r\n",
    "    except AssertionError:\r\n",
    "        return \"20201231\"\r\n",
    "\r\n",
    "def get_next_n_trade_date(trade_date, n=1):\r\n",
    "    trade_date=str(trade_date)[0:10].replace('-','')\r\n",
    "    pos = np.searchsorted(trade_dates_all, trade_date, side='right')\r\n",
    "    try:\r\n",
    "        assert pos + n - 1 < len(trade_dates_all)\r\n",
    "        return str(trade_dates_all[pos + n - 1])\r\n",
    "    except AssertionError:\r\n",
    "        return \"20201231\"\r\n",
    "    \r\n",
    "def get_date_stop_pl(df,ratio = 0.1):\r\n",
    "    sid = df[\"sid\"].iloc[0]\r\n",
    "    date = df[\"date\"].iloc[0]\r\n",
    "    hq_data = get_stk_bar(sid,start=date,end=get_next_n_trade_date(date,21),fields=[\"adj_close\",'adj_high',\"adj_low\"])\r\n",
    "    thr_up= hq_data[\"adj_close\"].values[0]*(1+ratio)\r\n",
    "    thr_down= hq_data[\"adj_close\"].values[0]*(1-ratio)\r\n",
    "    \r\n",
    "    df=pd.DataFrame(index=[0],columns=[\"date_{}p_ud\".format(int(ratio*100)),\"up_down\"])\r\n",
    "    if len(hq_data)<=1:\r\n",
    "        df.iloc[0]=(str(date)[0:10],0)\r\n",
    "        return df\r\n",
    "    \r\n",
    "    for dt in hq_data.index[1:21]:\r\n",
    "        if hq_data.loc[dt,\"adj_high\"] > thr_up:\r\n",
    "            df.iloc[0]=(str(dt)[0:10], 1)\r\n",
    "            return df\r\n",
    "        if hq_data.loc[dt,\"adj_low\"] < thr_down:\r\n",
    "            df.iloc[0]=(str(dt)[0:10],-1)\r\n",
    "            return df\r\n",
    "    \r\n",
    "    change_rate = hq_data.loc[dt,\"adj_close\"]/hq_data[\"adj_close\"].values[0]-1\r\n",
    "    df.iloc[0]=(str(dt)[0:10],change_rate)\r\n",
    "    return df\r\n",
    "\r\n",
    "def date_filter(df):\r\n",
    "    use_dates=[]\r\n",
    "    date_list = df[\"date\"].values\r\n",
    "    while(1):\r\n",
    "        if len(date_list)<=1:\r\n",
    "            break\r\n",
    "        else:\r\n",
    "            date =  min(date_list)\r\n",
    "            use_dates.append(date)\r\n",
    "            dates_rm = get_trade_dates(date,min(get_next_n_trade_date(date,19),\"20201231\"))\r\n",
    "            date_list=list(set(date_list)-set(dates_rm))\r\n",
    "            #print(len(date_list))\r\n",
    "    return df[df[\"date\"].isin(use_dates)]\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#更新白马\r\n",
    "date = \"str(datetime.datetime.now().date()).replace('-','')\"\r\n",
    "pre_date = get_previous_trade_date(date)\r\n",
    "stock_pool=pd.read_csv(r\"/share/xfzhang/to_colleague/to_yzhao/task2/{}/{}/task2_{}.csv\".format(pre_date[0:4],pre_date[4:6],pre_date[0:8]))[\"sid\"]\r\n",
    "cross_grid_today=pd.read_excel(r'/home/ywang/proj_cross_grid/result/cross_count_b240_baima/crosscount_summary_{}.xlsx'.format(date))\r\n",
    "cross_grid_today=cross_grid_today.set_index(\"sid\").loc[stock_pool].reset_index()[['sid','5d_count','10d_count']]\r\n",
    "cross_grid_today = cross_grid_today[(cross_grid_today[\"5d_count\"]>=20) & (cross_grid_today[\"10d_count\"]>=40)]\r\n",
    "cross_grid_today['date']=date\r\n",
    "\r\n",
    "cross_grid_summary = pd.read_excel(r\"result/穿网格五天大于20十天大于40_白马_全部.xlsx\",dtype={\"date\":str})\r\n",
    "date_bound = get_prev_n_trade_date(date,21)\r\n",
    "cross_grid_summary_keep = cross_grid_summary[cross_grid_summary[\"date\"]<date_bound]\r\n",
    "cross_grid_summary_to_renew = cross_grid_summary[cross_grid_summary[\"date\"]>=date_bound]\r\n",
    "\r\n",
    "cross_grid_summary_to_renew=cross_grid_summary_to_renew.drop(['date_10p_ud','up_down'],axis=1)\r\n",
    "cross_grid_summary_to_renew = pd.concat([cross_grid_summary_to_renew,cross_grid_today])\r\n",
    "cross_grid_summary_to_renew = cross_grid_summary_to_renew.drop_duplicates([\"sid\",\"date\"])\r\n",
    "group = cross_grid_summary_to_renew.groupby([\"sid\",\"date\"])\r\n",
    "df_date_10p_ud=apply_parallel(group,get_date_stop_pl).reset_index().\\\r\n",
    "                rename(columns={'level_0':'sid',\"level_1\":\"date\"}).drop(['level_2'],axis=1)\r\n",
    "cross_grid_summary_to_renew=pd.merge(cross_grid_summary_to_renew,df_date_10p_ud,on=[\"sid\",\"date\"])\r\n",
    "cross_grid_summary_raw = pd.concat([cross_grid_summary_keep,cross_grid_summary_to_renew])\r\n",
    "cross_grid_summary_raw =cross_grid_summary_raw.sort_values([\"date\",\"sid\"])\r\n",
    "cross_grid_summary_raw.to_excel(r\"result/穿网格五天大于20十天大于40_白马_全部.xlsx\",index=False)\r\n",
    "\r\n",
    "\r\n",
    "cross_grid_summary_filtered = cross_grid_summary_raw.groupby(\"sid\").apply(date_filter).reset_index(drop=True)\r\n",
    "cross_grid_summary_filtered =cross_grid_summary_filtered.sort_values([\"date\",\"sid\"])\r\n",
    "cross_grid_summary_filtered.to_excel(r\"result/穿网格五天大于20十天大于40_白马_20天内不重复触发.xlsx\",index=False)\r\n",
    "\r\n",
    "#更新底仓\r\n",
    "\r\n",
    "date = str(datetime.datetime.now().date()).replace('-','')\r\n",
    "pre_date = get_previous_trade_date(date)\r\n",
    "stock_pool=pd.read_csv(r\"/share/xfzhang/to_colleague/to_yzhao/task2_v2/{}/{}/task2_v2_{}.csv\".format(pre_date[0:4],pre_date[4:6],pre_date[0:8]))[\"sid\"]\r\n",
    "\r\n",
    "df_today_price=query_table(\"DailyBar\",date,date,fields=[\"pre_close\",\"low\"])\r\n",
    "df_today_price=df_today_price.set_index(\"sid\").loc[stock_pool.values].reset_index()\r\n",
    "df_today_price[\"intraday_decre\"] = df_today_price[\"low\"] / df_today_price[\"pre_close\"]-1\r\n",
    "stock_pool = df_today_price[[\"sid\",\"intraday_decre\"]][df_today_price[\"intraday_decre\"]<-0.03]\r\n",
    "\r\n",
    "cross_grid_today=pd.read_excel(r'/home/ywang/proj_cross_grid/result/cross_count_b240/crosscount_summary_{}.xlsx'.format(date))\r\n",
    "cross_grid_today=cross_grid_today.merge(stock_pool,on='sid')[['sid','5d_count','10d_count',\"intraday_decre\"]]\r\n",
    "cross_grid_today = cross_grid_today[(cross_grid_today[\"5d_count\"]>=50) & (cross_grid_today[\"10d_count\"]>=100)]\r\n",
    "cross_grid_today[\"date\"]=date\r\n",
    "\r\n",
    "cross_grid_summary = pd.read_excel(r\"result/穿网格五天大于50十天大于100盘中跌超3%_底仓_全部.xlsx\",dtype={\"date\":str})\r\n",
    "date_bound = get_prev_n_trade_date(date,21)\r\n",
    "cross_grid_summary_keep = cross_grid_summary[cross_grid_summary[\"date\"]<date_bound]\r\n",
    "cross_grid_summary_to_renew = cross_grid_summary[cross_grid_summary[\"date\"]>=date_bound]\r\n",
    "\r\n",
    "\r\n",
    "cross_grid_summary_to_renew=cross_grid_summary_to_renew.drop(['date_10p_ud','up_down'],axis=1)\r\n",
    "cross_grid_summary_to_renew = pd.concat([cross_grid_summary_to_renew,cross_grid_today])\r\n",
    "cross_grid_summary_to_renew = cross_grid_summary_to_renew.drop_duplicates([\"sid\",\"date\"])\r\n",
    "group = cross_grid_summary_to_renew.groupby([\"sid\",\"date\"])\r\n",
    "df_date_10p_ud=apply_parallel(group,get_date_stop_pl).reset_index().\\\r\n",
    "                rename(columns={'level_0':'sid',\"level_1\":\"date\"}).drop(['level_2'],axis=1)\r\n",
    "cross_grid_summary_to_renew=pd.merge(cross_grid_summary_to_renew,df_date_10p_ud,on=[\"sid\",\"date\"])\r\n",
    "cross_grid_summary_raw = pd.concat([cross_grid_summary_keep,cross_grid_summary_to_renew])\r\n",
    "cross_grid_summary_raw =cross_grid_summary_raw.sort_values([\"date\",\"sid\"])\r\n",
    "cross_grid_summary_raw.to_excel(r\"result/穿网格五天大于50十天大于100盘中跌超3%_底仓_全部.xlsx\",index=False)\r\n",
    "\r\n",
    "cross_grid_summary_filtered = cross_grid_summary_raw.groupby(\"sid\").apply(date_filter).reset_index(drop=True)\r\n",
    "cross_grid_summary_filtered =cross_grid_summary_filtered.sort_values([\"date\",\"sid\"])\r\n",
    "cross_grid_summary_filtered.to_excel(r\"result/穿网格五天大于50十天大于100盘中跌超3%_底仓_20天内不重复触发.xlsx\",index=False)\r\n",
    "\r\n",
    "logging.info(\"finish\"+str(date))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  40 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=16)]: Done 130 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=16)]: Done 616 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=16)]: Done 850 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=16)]: Done 1120 tasks      | elapsed:   33.3s\n",
      "[Parallel(n_jobs=16)]: Done 1344 out of 1344 | elapsed:   39.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  40 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=16)]: Done 130 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=16)]: Done 616 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=16)]: Done 850 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=16)]: Done 1120 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=16)]: Done 1417 out of 1417 | elapsed:   43.0s finished\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%run stock_pool_renew.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  40 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=16)]: Done 130 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=16)]: Done 616 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=16)]: Done 850 tasks      | elapsed:   27.6s\n",
      "[Parallel(n_jobs=16)]: Done 1120 tasks      | elapsed:   36.1s\n",
      "[Parallel(n_jobs=16)]: Done 1195 out of 1195 | elapsed:   38.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  40 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=16)]: Done 130 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=16)]: Done 616 tasks      | elapsed:   20.4s\n",
      "[Parallel(n_jobs=16)]: Done 850 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=16)]: Done 1120 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=16)]: Done 1426 tasks      | elapsed:   46.6s\n",
      "[Parallel(n_jobs=16)]: Done 1635 out of 1635 | elapsed:   53.3s finished\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import pandas as pd\r\n",
    "temp_df = pd.read_excel(r\"result/穿网格五天大于50十天大于100盘中跌超3%_底仓_全部.xlsx\")\r\n",
    "temp_df=temp_df[temp_df[\"date\"]<20191212]\r\n",
    "\r\n",
    "temp_df_part2 = pd.read_excel(r\"result/穿网格五天大于50十天大于100盘中跌超3%_底仓_全部_2.xlsx\")\r\n",
    "temp_df_part2=temp_df_part2.sort_values([\"date\",\"sid\"])\r\n",
    "\r\n",
    "temp_df= pd.concat([temp_df,temp_df_part2])\r\n",
    "temp_df.to_excel(r\"result/穿网格五天大于50十天大于100盘中跌超3%_底仓_全部.xlsx\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}