{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing module\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "code=\"601211.SH\"\n",
    "date=20191023\n",
    "pre_date=20191022\n",
    "\n",
    "close_seq=np.array(data.loc[(code,date)][\"close\"])\n",
    "pre_close_seq=np.array(data.loc[(code,pre_date)][\"close\"][-60:])\n",
    "pre_close=data.loc[(code,date)][\"pre_close\"][0]\n",
    "today_open=data.loc[(code,date)][\"open\"][0]\n",
    "\n",
    "plt.plot(pre_close_seq)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(close_seq)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(cross_count(pre_close_seq,close_seq,pre_close))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/ywang/env/dev/bin\n",
    "#-*-coding:utf-8-*-\n",
    "# written by wangyonghao\n",
    "\n",
    "# the api: get_daily_cross_data(date)\n",
    "\n",
    "# input:  date: current date ( \"20190102\" for example)\n",
    "\n",
    "# output to file: the csv file of the cross_grid_count for each stock\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from fb_base_get_market_data import *\n",
    "from data_tools.api import *\n",
    "from utilscht.Data import *\n",
    "import pymysql\n",
    "import datetime\n",
    "import logging\n",
    "import click\n",
    "from reader.reader import BarReader\n",
    "\n",
    "def get_cross_grid_info(price_now, ref_price,grids):\n",
    "    if price_now > ref_price:\n",
    "        cross_grids = [i for i in grids if i > ref_price and i <= price_now]\n",
    "        if len(cross_grids) > 0:\n",
    "            grids_num = len(cross_grids)\n",
    "            direction = 1\n",
    "            last_grid = cross_grids[-1]\n",
    "\n",
    "    else:\n",
    "        cross_grids = [i for i in grids if i > price_now and i <= ref_price]\n",
    "        if len(cross_grids) > 0:\n",
    "            grids_num = len(cross_grids)\n",
    "            direction = -1\n",
    "            last_grid = cross_grids[0]\n",
    "\n",
    "    if len(cross_grids) > 0:\n",
    "        return (grids_num, direction, last_grid)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# 输入前一天最后 60min 的close序列、今天的全部close序列、前一天的收盘价、今天的开盘价\n",
    "# ，返回今天穿越网格线的次数（一段时间内重复穿过同一条线只计算一次）\n",
    "def get_cross_count(pre_close_seq, close_seq):\n",
    "    total_seq=np.concatenate([pre_close_seq,close_seq])\n",
    "    if np.isnan(total_seq).sum()>10:\n",
    "        return np.nan\n",
    "    \n",
    "    if (np.nanmin(total_seq)+np.nanmax(total_seq))/2>=10:\n",
    "        grid_unit=(np.nanmin(total_seq)+np.nanmax(total_seq))/2*0.01\n",
    "    else:\n",
    "        grid_unit = 0.1\n",
    "    grid_num = int((np.nanmax(total_seq)-np.nanmin(total_seq))/grid_unit+3)\n",
    "    grids = np.linspace(np.nanmin(total_seq)-grid_unit,np.nanmax(total_seq)+grid_unit,grid_num)\n",
    "    \n",
    "    count = 0\n",
    "    ref_price = 0\n",
    "    pre_cross_dire = 1\n",
    "\n",
    "    # get the last corss_line for yesterday\n",
    "    for i in reversed(range(len(pre_close_seq))):\n",
    "        price_pre = pre_close_seq[i - 1]\n",
    "        price_now = pre_close_seq[i]\n",
    "        cross_grid_info = get_cross_grid_info(price_now, price_pre,grids)\n",
    "\n",
    "        if cross_grid_info != -1:\n",
    "            ref_price = cross_grid_info[2]\n",
    "            pre_cross_dire = cross_grid_info[1]\n",
    "            break\n",
    "\n",
    "    # count the cross times for today\n",
    "    close_seq = np.concatenate([np.array([pre_close_seq[-1]]),close_seq])\n",
    "    for i in range(len(close_seq) - 1):\n",
    "        price_pre = close_seq[i]\n",
    "        price_now = close_seq[i + 1]\n",
    "\n",
    "        cross_grid_info = get_cross_grid_info(price_now, price_pre,grids)\n",
    "\n",
    "        if cross_grid_info == -1:\n",
    "            continue\n",
    "\n",
    "        # judge the status of \"now\" and \"pre\"\n",
    "        grids_num = cross_grid_info[0]\n",
    "        cross_dire = cross_grid_info[1]\n",
    "        last_grid = cross_grid_info[2]\n",
    "\n",
    "        if last_grid != ref_price:\n",
    "            if cross_dire * pre_cross_dire == 1:\n",
    "                count = count + grids_num\n",
    "            else:\n",
    "                count = count + grids_num - 1\n",
    "\n",
    "        ref_price = last_grid\n",
    "        pre_cross_dire = cross_dire\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "# current function to get 1min_freq data\n",
    "def get_stk_data(sid, date):\n",
    "    \n",
    "    try:\n",
    "        DTYPE_STK_BAR_1MIN = [\n",
    "            ('wcode', 'S16'),\n",
    "            ('date', 'i4'),\n",
    "            ('time', 'i4'),\n",
    "            ('open', 'i4'),\n",
    "            ('high', 'i4'),\n",
    "            ('low', 'i4'),\n",
    "            ('close', 'i4'),\n",
    "            ('volume', 'i8'),\n",
    "            ('value', 'i8'),\n",
    "            ('num_trades', 'i4'),\n",
    "            ('unused', 'i4'),\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        bar=np.memmap(r\"/dat/raw/wind/stk_bar_1min/{}/{}.dat\".format(date,sid),dtype=DTYPE_STK_BAR_1MIN,mode=\"r\")\n",
    "        df = pd.DataFrame(bar)[[\"wcode\",\"date\",\"time\",\"close\"]]\n",
    "        df[\"sid\"] = df[\"wcode\"].apply(lambda x:str(x)[2:-1])\n",
    "        df[\"DataDate\"] = df[\"date\"].apply(str)\n",
    "        df[\"close\"] = df[\"close\"]/10000\n",
    "        df[\"pre_close\"] = get_stk_bar(sid, freq=\"1d\", start=date, end=date, fields=[\"pre_close\"]).values[0][0]\n",
    "        df = df.drop([\"wcode\",\"date\",\"time\"],axis=1)\n",
    "        df[\"ticktime\"] = df.index\n",
    "        return df[[\"sid\",\"DataDate\",\"ticktime\",\"close\",\"pre_close\"]]\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            stock_data = get_stk_bar(sid, freq=\"1m\", start=date, end=date, fields=[\"open\", \"high\", \"low\", \"close\"])\n",
    "            stock_data = stock_data.reset_index().rename(columns={\"index\": \"datetime\"})\n",
    "            stock_data[\"ticktime\"] = stock_data[\"datetime\"].index\n",
    "            stock_data[\"DataDate\"] = stock_data[\"datetime\"].apply(lambda x:str(x.date()))\n",
    "            del stock_data[\"datetime\"]\n",
    "            stock_data[\"pre_close\"] = get_stk_bar(sid, freq=\"1d\", start=date, end=date, fields=[\"pre_close\"]).values[0][0]\n",
    "            stock_data[\"sid\"] = sid\n",
    "            return stock_data\n",
    "        except:\n",
    "            logging.warning(\"Data Lost for {} on {}\".format(sid,date))\n",
    "\n",
    "\n",
    "def get_daily_data(date):\n",
    "    df=get_db(start_date = date, end_date = date,cols=[\"trade_status\"])\n",
    "    stk_ls=list(df[\"sid\"][df[\"trade_status\"]==1])\n",
    "\n",
    "    results = Parallel(n_jobs=16, verbose=5, backend=\"loky\", batch_size='auto') \\\n",
    "        (delayed(get_stk_data)(sid, date) for sid in stk_ls)\n",
    "    data = pd.concat(results)\n",
    "\n",
    "    return data.sort_values([\"sid\", \"ticktime\"])\n",
    "\n",
    "def get_today_stk_data(sid,date):\n",
    "    try:\n",
    "        with BarReader('/home/ywang/proj_cross_grid/reader/bar.yaml') as client:\n",
    "            bars = client.stock_bars(sid[0:6])\n",
    "            df = pd.DataFrame(bars)\n",
    "            df[\"sid\"] = df[\"symbol\"].apply(lambda x:str(x)[2:-1])\n",
    "            df[\"DataDate\"] = df[\"date\"].apply(str)\n",
    "            df[\"ticktime\"] = df.index\n",
    "            df = df.drop([\"symbol\",\"date\",\"time\"],axis=1)\n",
    "            df[\"pre_close\"] = df[\"preclose\"].iloc[0]\n",
    "            df= df[df[\"sid\"] != \"\"]\n",
    "            return df[[\"sid\",\"DataDate\",\"ticktime\",\"close\",\"pre_close\"]]\n",
    "    except:\n",
    "        logging.warning(\"Data Lost for {} on {}\".format(sid,date))\n",
    "\n",
    "\n",
    "def get_today_data(date):\n",
    "    pre_date = get_previous_trade_date(date)\n",
    "    df=get_db(start_date = pre_date, end_date = pre_date,cols=[\"trade_status\"])\n",
    "    stk_ls=list(df[\"sid\"][df[\"trade_status\"] == 1])\n",
    "\n",
    "    results = Parallel(n_jobs=16, verbose=5, backend=\"loky\", batch_size='auto') \\\n",
    "        (delayed(get_today_stk_data)(sid,date) for sid in stk_ls)\n",
    "    data = pd.concat(results)\n",
    "\n",
    "    return data.sort_values([\"sid\", \"ticktime\"])\n",
    "\n",
    "\n",
    "def get_count_parallel(df):\n",
    "    if sum(pd.isnull(df[\"yesterday_close\"]))>120:\n",
    "        return pd.DataFrame()\n",
    "    sid = df[\"sid\"].drop_duplicates()\n",
    "    temp_df = pd.DataFrame(index=sid, columns=[\"cross_count\"])\n",
    "    \n",
    "    pre_close = df[\"pre_close\"].iloc[0]\n",
    "    if pre_close!=df[\"yesterday_close\"].iloc[-1]:\n",
    "        df[\"yesterday_close\"] = df[\"yesterday_close\"]* pre_close/df[\"yesterday_close\"].iloc[-1]\n",
    "\n",
    "    close_seq = np.concatenate([np.array(df[\"yesterday_close\"].iloc[-40:]),\\\n",
    "                                np.array(df[\"close\"].iloc[:200])])\n",
    "    close_seq = close_seq[~np.isnan(close_seq)]\n",
    "    pre_close_seq = np.array(df[\"yesterday_close\"].iloc[-240:-40])\n",
    "    \n",
    "    count = get_cross_count(pre_close_seq, close_seq)\n",
    "\n",
    "    temp_df.iloc[0, 0] = count\n",
    "    return temp_df\n",
    "\n",
    "\n",
    "def apply_parallel(df_grouped, func, n_jobs=16, backend='loky', as_index=False, **kwargs):\n",
    "    \"\"\"\n",
    "    This is for parallel between grouped generated by pd.DataFrame.groupby\n",
    "    :param df_grouped:\n",
    "    :param func:\n",
    "    :param n_jobs:\n",
    "    :param backend:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    names = []\n",
    "    groups = []\n",
    "    for name, group in df_grouped:\n",
    "        names.append(name)\n",
    "        groups.append(group)\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=5, backend=backend, batch_size='auto') \\\n",
    "        (delayed(func)(group, **kwargs) for group in groups)\n",
    "\n",
    "    return pd.concat(results, keys=names if as_index else None)\n",
    "\n",
    "@click.command()\n",
    "@click.argument(\"date\",nargs=1)\n",
    "def get_daily_cross_data(date):\n",
    "    pre_date = get_previous_trade_date(date)\n",
    "\n",
    "    data = get_today_data(date)\n",
    "    data[\"sid\"] = data[\"sid\"].apply(lambda x:str(x)+\".SH\" if str(x)[0]=='6' else str(x)+'.SZ')\n",
    "    data[\"DataDate\"] = data[\"DataDate\"].apply(str)\n",
    "    yesterday_data = get_daily_data(pre_date)\n",
    "    yesterday_data.rename(columns={\"close\": \"yesterday_close\"}, inplace=True)\n",
    "    data = pd.merge(data, yesterday_data[[\"sid\", \"ticktime\", \"yesterday_close\"]], how=\"left\", on=[\"sid\", \"ticktime\"])\n",
    "\n",
    "    groups = data.groupby(\"sid\")\n",
    "    cross_count_df = apply_parallel(groups, get_count_parallel)\n",
    "    cross_count_df.to_csv(r\"/home/ywang/proj_cross_grid/result/cross_count_b240/crosscount_{}.csv\".format(date), index_label=\"sid\")    \n",
    "\n",
    "def get_daily_cross_data_hist(date):\n",
    "    pre_date = get_previous_trade_date(date)\n",
    "\n",
    "    data = get_daily_data(date)\n",
    "    yesterday_data = get_daily_data(pre_date)\n",
    "    yesterday_data.rename(columns={\"close\": \"yesterday_close\"}, inplace=True)\n",
    "    data = pd.merge(data, yesterday_data[[\"sid\", \"ticktime\", \"yesterday_close\"]], how=\"left\", on=[\"sid\", \"ticktime\"])\n",
    "    \n",
    "    print(data.head())\n",
    "    \n",
    "    groups = data.groupby(\"sid\")\n",
    "    cross_count_df = apply_parallel(groups, get_count_parallel)\n",
    "    cross_count_df.to_csv(r\"/home/ywang/proj_cross_grid/result/cross_count_b240/crosscount_{}.csv\".format(date), index_label=\"sid\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  40 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=16)]: Done 216 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=16)]: Done 720 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=16)]: Done 1368 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=16)]: Done 2160 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=16)]: Done 3096 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=16)]: Done 3783 out of 3814 | elapsed:   16.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 3814 out of 3814 | elapsed:   16.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  40 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=16)]: Done 184 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=16)]: Done 688 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=16)]: Done 1336 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=16)]: Done 2128 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=16)]: Done 3064 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=16)]: Done 3782 out of 3813 | elapsed:   16.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 3813 out of 3813 | elapsed:   16.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sid  DataDate  ticktime  close  pre_close  yesterday_close\n",
      "0  000001.SZ  20200521         0  13.54      13.51            13.32\n",
      "1  000001.SZ  20200521         1  13.52      13.51            13.30\n",
      "2  000001.SZ  20200521         2  13.54      13.51            13.27\n",
      "3  000001.SZ  20200521         3  13.51      13.51            13.30\n",
      "4  000001.SZ  20200521         4  13.50      13.51            13.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  40 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=16)]: Done 672 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=16)]: Done 1320 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=16)]: Done 2112 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=16)]: Done 3048 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=16)]: Done 3707 out of 3707 | elapsed:   11.4s finished\n"
     ]
    }
   ],
   "source": [
    "get_daily_cross_data_hist(\"20200521\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_parallel(df_grouped, func, n_jobs=16, backend='loky', as_index=True, **kwargs):\n",
    "    \"\"\"\n",
    "    This is for parallel between grouped generated by pd.DataFrame.groupby\n",
    "    :param df_grouped:\n",
    "    :param func:\n",
    "    :param n_jobs:\n",
    "    :param backend:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    names = []\n",
    "    groups = []\n",
    "    for name, group in df_grouped:\n",
    "        names.append(name)\n",
    "        groups.append(group)\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=5, backend=backend, batch_size='auto') \\\n",
    "        (delayed(func)(group, **kwargs) for group in groups)\n",
    "\n",
    "    return pd.concat(results, keys=names if as_index else None)\n",
    "\n",
    "\n",
    "\n",
    "def get_period_stk_data(sid,start_date,end_date):\n",
    "    try:\n",
    "        stock_data = get_stk_bar(sid, freq=\"1m\", start=start_date, end=end_date, fields=[\"open\", \"high\", \"low\", \"close\"])\n",
    "        stock_data = stock_data.reset_index().rename(columns={\"index\": \"datetime\"})\n",
    "        stock_data[\"DataDate\"]=stock_data[\"datetime\"].apply(lambda x:str(x)[0:10].replace('-',''))\n",
    "        df_pre_close = get_stk_bar(sid, freq=\"1d\", start=start_date, end=end_date, fields=[\"pre_close\"])\n",
    "        df_pre_close = df_pre_close.reset_index().rename(columns={\"date\": \"datetime\"})\n",
    "        df_pre_close[\"DataDate\"]=df_pre_close[\"datetime\"].apply(lambda x:str(x)[0:10].replace('-',''))\n",
    "        df_pre_close = df_pre_close.drop([\"datetime\"],axis=1)\n",
    "        stock_data = stock_data.merge(df_pre_close,on=\"DataDate\")\n",
    "        stock_data[\"sid\"] = sid\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return pd.DataFrame()\n",
    "    return stock_data\n",
    "\n",
    "def get_period_data(start_date, end_date):\n",
    "    df=get_db(start_date=start_date,end_date=end_date,cols=[\"trade_status\"])\n",
    "    df= df[df[\"trade_status\"]==1]\n",
    "    stk_ls = df[\"sid\"].drop_duplicates()\n",
    "\n",
    "    results = Parallel(n_jobs=16, verbose=5, backend=\"loky\", batch_size='auto') \\\n",
    "        (delayed(get_period_stk_data)(sid, start_date,end_date) for sid in stk_ls)\n",
    "    data = pd.concat(results)\n",
    "\n",
    "    return data.sort_values([\"sid\", \"datetime\"])\n",
    "\n",
    "def append_yesterday_data(df):\n",
    "    df[\"yesterday_close\"] = df[\"close\"].shift(240)\n",
    "    return df.iloc[240:]\n",
    "\n",
    "def data_store(df):\n",
    "    date=df[\"DataDate\"].iloc[0]\n",
    "    df = df.drop([\"DataDate\"],axis=1)\n",
    "    df.to_csv(r\"/home/ywang/proj_cross_grid/result/cross_count_b240/crosscount_{}.csv\".format(date),index=False)\n",
    "    return 0\n",
    "\n",
    "def get_period_cross_data(start_date, end_date):\n",
    "    data=get_period_data(get_previous_trade_date(start_date),end_date)\n",
    "    \n",
    "    data = data.groupby(\"sid\",as_index = False).apply(append_yesterday_data)\n",
    "\n",
    "    groups = data.groupby([\"sid\",\"DataDate\"])\n",
    "    cross_count_df = apply_parallel(groups, get_count_parallel)\n",
    "    cross_count_df = cross_count_df.reset_index().drop([\"sid\"],axis=1).rename(columns={\"level_0\":\"sid\",\"level_1\":\"DataDate\"})\n",
    "    cross_count_df.groupby(\"DataDate\").apply(data_store)\n",
    "    return cross_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=get_period_data(get_previous_trade_date(\"20200507\"),\"20200508\")\n",
    "\n",
    "data = data.groupby(\"sid\",as_index = False).apply(append_yesterday_data)\n",
    "\n",
    "groups = data.groupby([\"sid\",\"DataDate\"])\n",
    "cross_count_df = apply_parallel(groups, get_count_parallel)\n",
    "cross_count_df = cross_count_df.reset_index().drop([\"sid\"],axis=1).rename(columns={\"level_0\":\"sid\",\"level_1\":\"DataDate\"})\n",
    "cross_count_df.groupby(\"DataDate\").apply(data_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_period_cross_data(\"20200507\",\"20200508\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=get_period_data(\"20191220\",\"20200131\")\n",
    "\n",
    "data = data.groupby(\"sid\",as_index = False).apply(append_yesterday_data)\n",
    "\n",
    "groups = data.groupby([\"sid\",\"DataDate\"])\n",
    "cross_count_df = apply_parallel(groups, get_count_parallel)\n",
    "cross_count_df = cross_count_df.reset_index().drop([\"sid\"],axis=1).rename(columns={\"level_0\":\"sid\",\"level_1\":\"DataDate\"})\n",
    "cross_count_df.groupby(\"DataDate\").apply(data_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_ls = get_trade_dates('20180101','20191231')\n",
    "for i in range(0,len(date_ls),60):\n",
    "    try:\n",
    "        get_period_cross_data(date_ls[i],date_ls[i+59])\n",
    "    except IndexError:\n",
    "        get_period_cross_data(date_ls[i],'20191231')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#written by wangyonghao\n",
    "\n",
    "#the api: get_crosscount_summary(date,period_ls)\n",
    "\n",
    "#input: date: current date ( \"20190102\" for example)\n",
    "#       period_ls:the list for the window period ([5,10,15] for example)\n",
    "\n",
    "\n",
    "#output to file: the excel file of the cross_grid_count_summary for each window_period and for each stock\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tushare as ts\n",
    "import click\n",
    "\n",
    "def factor_data_calcu(td_date, stock_ls, window_period, factor_df, crosscount_df):\n",
    "    date_idx = date_ls.index(td_date)\n",
    "    date_range = date_ls[date_idx - window_period + 1:date_idx + 1]\n",
    "\n",
    "    count = np.sum(np.array(crosscount_df.loc[date_range, stock_ls]), axis=0)\n",
    "\n",
    "    factor_df.loc[stock_ls, \"{}d_count\".format(window_period)] = count\n",
    "\n",
    "def get_crosscount_summary(date,period_ls):\n",
    "\n",
    "    date_range = date_ls[max(1, date_ls.index(date) - 29):date_ls.index(date) + 1]\n",
    "    crosscount_df = pd.DataFrame()\n",
    "    for datadate in date_range:\n",
    "        temp_df = pd.read_csv(r\"/home/ywang/proj_cross_grid/result/cross_count_b240/crosscount_{}.csv\".format(datadate), index_col=\"sid\")\n",
    "        temp_df = temp_df.reindex(pd.MultiIndex.from_product([[datadate], temp_df.index]), level=1)\n",
    "        crosscount_df = pd.concat([crosscount_df, temp_df])\n",
    "    crosscount_df = crosscount_df.unstack()[\"cross_count\"]\n",
    "    crosscount_df = crosscount_df.dropna(axis=1, how=\"any\")\n",
    "\n",
    "    factor_df = pd.DataFrame(index=crosscount_df.columns,\n",
    "                             columns=[str(i)+\"d_count\" for i in period_ls])\n",
    "\n",
    "    for window_period in period_ls:\n",
    "        factor_data_calcu(date, crosscount_df.columns, window_period, factor_df, crosscount_df)\n",
    "    factor_df[\"mean_5-mean_30\"]=factor_df[\"5d_count\"]/5-factor_df[\"30d_count\"]/30\n",
    "    factor_df[\"mean_10-mean_30\"]=factor_df[\"10d_count\"]/10-factor_df[\"30d_count\"]/30\n",
    "\n",
    "    factor_df.to_excel(r\"/home/ywang/proj_cross_grid/result/cross_count_b240/crosscount_summary_{}.xlsx\".format(date),\n",
    "                       index_label=\"sid\")\n",
    "    \n",
    "    pre_date = date\n",
    "    stock_pool=pd.read_csv(r\"/share/xfzhang/to_colleague/to_yzhao/task2/{}/{}/task2_{}.csv\".\\\n",
    "                                        format(pre_date[0:4],pre_date[4:6],pre_date[0:8]))[\"sid\"]\n",
    "    factor_df_baima=factor_df.loc[stock_pool]\n",
    "    factor_df_baima.to_excel(r\"/home/ywang/proj_cross_grid/result/cross_count_b240_baima/crosscount_summary_{}.xlsx\"\\\n",
    "                             .format(date),index_label=\"sid\")\n",
    "    \n",
    "\n",
    "    print(date,\"finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ywang/env/dev/lib/python3.6/site-packages/pandas/core/indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n",
      "/home/ywang/env/dev/lib/python3.6/site-packages/ipykernel_launcher.py:51: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200511 finished\n",
      "20200512 finished\n",
      "20200513 finished\n",
      "20200514 finished\n",
      "20200515 finished\n",
      "20200518 finished\n",
      "20200519 finished\n",
      "20200520 finished\n",
      "20200521 finished\n"
     ]
    }
   ],
   "source": [
    "date_ls=get_trade_dates(20200325,20200521)\n",
    "date_ls=[str(i) for i in date_ls]\n",
    "for date in date_ls[29:]:\n",
    "    get_crosscount_summary(date,[5,10,15,20,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(r\"/home/ywang/proj_cross_grid/result/cross_count_b240/crosscount_summary_{}.xlsx\".format('20191212'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计 10d_ma_crosscount 大于10的股票\n",
    "\n",
    "date_ls=get_trade_dates(20190301,20191029)\n",
    "\n",
    "summary_df=pd.DataFrame()\n",
    "for date in date_ls:\n",
    "    temp_df=pd.read_excel(r\"/share/intern_share/stk_bundle/crosscount_summary_{}.xlsx\".format(date),index_col=\"sid\")\n",
    "    temp_df = temp_df.reindex(pd.MultiIndex.from_product([[date], temp_df.index]), level=1)\n",
    "    for window in [5,10,15,20,30]:\n",
    "        temp_df[\"{}d_ma>10\".format(window)]=temp_df[\"{}d_count\".format(window)]>10*window\n",
    "    \n",
    "    columns=[\"{}d_ma>10\".format(i) for i in [5,10,15,20,30]]\n",
    "    temp_df[\"any_ma>10\"]=temp_df[columns].apply(lambda x:any(x),axis=1)\n",
    "    summary_df=pd.concat([summary_df,temp_df])\n",
    "    \n",
    "summary_df=summary_df.reset_index().rename(columns={\"level_0\":\"date\";,\"level_1\":\"sid\"})\n",
    "\n",
    "sid_date_df=summary_df[[\"sid\",\"date\"]][summary_df[\"any_ma>10\"] == True]\n",
    "sid_date_df.reset_index(drop=True,inplace=True)\n",
    "sid_date_df.to_csv(r\"data/crosscount_ma_>10_stocklist.csv\",index=False)\n",
    "\n",
    "sid_date_df.to_csv(r\"/share/intern_share/assign_1030/crosscount_ma_>10_stocklist.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cross_count_summary_v240.py \"20200115\" 5 10 15 20 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
