{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_tools.api import *\n",
    "from utilscht.Data import *\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from joblib import Parallel,delayed\n",
    "import tushare as ts\n",
    "import datetime\n",
    "import math\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "START_YEAR=2014\n",
    "\n",
    "DB_INFO = dict(host='192.168.1.234',\n",
    "               user='winduser',\n",
    "               password='1qaz@WSX',\n",
    "               db='wind')\n",
    "\n",
    "conn = pymysql.connect(**DB_INFO, charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_parallel(df_grouped, func, n_jobs=16, backend='loky', as_index=True, **kwargs):\n",
    "    \"\"\"\n",
    "    This is for parallel between grouped generated by pd.DataFrame.groupby\n",
    "    :param df_grouped:\n",
    "    :param func:\n",
    "    :param n_jobs:\n",
    "    :param backend:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    names = []\n",
    "    groups = []\n",
    "    for name, group in df_grouped:\n",
    "        names.append(name)\n",
    "        groups.append(group)\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=5, backend=backend, batch_size='auto') \\\n",
    "        (delayed(func)(group, **kwargs) for group in groups)\n",
    "\n",
    "    return pd.concat(results, keys=names if as_index else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "+ 计算上一年所有研报对已发布年报的偏差\n",
    "+ 将偏差对市值、行业、距离年末时间进行回归\n",
    "+ 取回归残差作为分析师自身偏差\n",
    "\n",
    "2. \n",
    "+ 对过去3个月（过去三个月研报数小于5的个股使用过去6月的数据，对于过去六个月没有研报的个股填充nan值）\n",
    " 的数据进行过滤，同一分析师对同一个股的多篇研报只取最后一篇\n",
    "\n",
    "3. \n",
    "+ 对过去三个月的一致预期数据按照时间和分析师偏差进行双重加权（时间越近权重越大，分析师偏差越小权重越大）\n",
    "\n",
    "4. \n",
    "+ 如果企业发布了业绩预报或者快报，则直接采取业绩预报或者快报的数据\n",
    "\n",
    "5.                  \n",
    "+ 附：滚动指标计算（向未来一年进行滚动）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot_share[df_tot_share[\"sid\"]=='600380.SH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_FA_EPS[df_FA_EPS[\"sid\"]=='600380.SH'].sort_values(\"REPORTING_PERIOD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df = df_est[(df_est[\"sid\"]=='000761.SZ') & (df_est[\"REPORTING_PERIOD\"] == '20211231')]\n",
    "temp_df=temp_df[temp_df[\"DataDate\"]>'20181110']\n",
    "df = Process_Single_StkPeriod(temp_df)\n",
    "plt.plot(pd.to_datetime(df.index),np.array(df[\"con_est_eps\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df = df_est[(df_est[\"sid\"]=='000016.SZ') & (df_est[\"REPORTING_PERIOD\"] == '20191231')]\n",
    "temp_df=temp_df[temp_df[\"DataDate\"]>'20181118']\n",
    "temp_df[[\"sid\",\"DataDate\",\"EST_EPS_DILUTED\",\"analyst_avg_bias\"]].iloc[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##计算上一年所有研报对已发布年报的偏差\n",
    "\n",
    "#获取行业和市值\n",
    "indus_size_df=query_table(\"DailyBar\",start_date=\"{}0101\".format(START_YEAR),end_date=\"20201231\",fields=[\"L1_INDUSTRY\",\"mktcap\"])\n",
    "indus_size_df[\"DataDate\"]=indus_size_df[\"DataDate\"].apply(lambda x:str(x))\n",
    "\n",
    "#获取研报预期数据\n",
    "sql = \"\"\"SELECT S_INFO_WINDCODE, EST_DT, REPORTING_PERIOD,RESEARCH_INST_NAME, ANALYST_NAME,EST_NET_PROFIT \n",
    "from ASHAREEARNINGEST where EST_DT between '{}0101' and '20201231'\"\"\".format(START_YEAR)   \n",
    "df_est = pd.read_sql_query(sql, conn)\n",
    "df_est.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"EST_DT\":\"DataDate\"},inplace=True)\n",
    "df_est=df_est.sort_values([\"sid\",\"DataDate\",\"REPORTING_PERIOD\"])\n",
    "df_est=pd.merge(df_est,indus_size_df,on=[\"sid\",\"DataDate\"],how=\"left\")\n",
    "\n",
    "#获取发布的年报数据\n",
    "sql = \"\"\"SELECT S_INFO_WINDCODE, ANN_DT, REPORT_PERIOD,S_FA_DEDUCTEDPROFIT,S_FA_EXTRAORDINARY\n",
    "from ASHAREFINANCIALINDICATOR where ANN_DT between '{}0101' and '20201231'\"\"\".format(START_YEAR)\n",
    "df_FA_EPS = pd.read_sql_query(sql, conn)\n",
    "df_FA_EPS.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"REPORT_PERIOD\":\"REPORTING_PERIOD\"},inplace=True)\n",
    "df_est=pd.merge(df_est,df_FA_EPS,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "\n",
    "#获取总股本信息\n",
    "sql=\"select S_INFO_WINDCODE,TOT_SHR,CHANGE_DT from AShareCapitalization\".upper()\n",
    "df_tot_share=pd.read_sql_query(sql,conn)\n",
    "df_tot_share.rename(columns={\"S_INFO_WINDCODE\":\"sid\"},inplace=True)\n",
    "df_tot_share=df_tot_share.sort_values([\"sid\",\"CHANGE_DT\"])\n",
    "df_now_tot_share=df_tot_share.groupby('sid',as_index=False).apply(lambda x:x.iloc[-1])\n",
    "\n",
    "#计算前复权每股预期收益\n",
    "df_est=pd.merge(df_est,df_now_tot_share,on='sid',how='left')\n",
    "df_est['EST_EPS_DILUTED']=df_est['EST_NET_PROFIT']/df_est['TOT_SHR']\n",
    "df_est['S_FA_EPS_DILUTED']=(df_est['S_FA_DEDUCTEDPROFIT']+df_est['S_FA_EXTRAORDINARY'])/df_est['TOT_SHR']/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算距离年末的月数（第二年初的预测月数为负值）\n",
    "df_1y_est=df_est.groupby([\"sid\",\"DataDate\",\"ANALYST_NAME\"],as_index=False).apply(lambda x:x.iloc[0]).reset_index(drop=True)\n",
    "df_1y_est[\"Date_to_Reporting\"]=df_1y_est[[\"DataDate\",\"REPORTING_PERIOD\"]].apply(lambda x:int(x[1][4:6])-int(x[0][4:6]) if x[0][0:4]==x[1][0:4] else int(x[1][4:6])-int(x[0][4:6])-12,axis=1)\n",
    "\n",
    "#计算偏差比例\n",
    "df_1y_est[\"Analyst_Bias\"]=np.abs(df_1y_est[\"S_FA_EPS_DILUTED\"]-df_1y_est[\"EST_EPS_DILUTED\"])/df_1y_est[\"S_FA_EPS_DILUTED\"]\n",
    "\n",
    "#按照年度将偏差对行业、市值、距离年末月数进行回归取残差\n",
    "def Regress_by_Year(df):\n",
    "    df=df[pd.notnull(df[\"Analyst_Bias\"])&pd.notnull(df[\"L1_INDUSTRY\"])]\n",
    "    if len(df)==0:\n",
    "        return df\n",
    "    bias=np.array(df[\"Analyst_Bias\"])\n",
    "    \n",
    "    indus_dummy=np.array(pd.get_dummies(df[\"L1_INDUSTRY\"]))\n",
    "    mktv=np.array(df[\"mktcap\"])\n",
    "    date_to_reporting=np.array(df[\"Date_to_Reporting\"])\n",
    "    \n",
    "    model=sm.OLS(bias,np.column_stack([indus_dummy,mktv,date_to_reporting]))\n",
    "    result=model.fit()\n",
    "    df[\"adj_analyst_bias\"]=result.resid\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_1y_est[\"adj_analyst_bias\"]=np.nan\n",
    "df_1y_est=df_1y_est.groupby(\"REPORTING_PERIOD\",as_index=False).apply(lambda x:Regress_by_Year(x)).reset_index(drop=True)\n",
    "\n",
    "#按照预测年度、分析师来计算偏差平均值作为下一年度该分析师的可信水平\n",
    "analyst_avg_bias=df_1y_est.groupby([\"REPORTING_PERIOD\",\"ANALYST_NAME\"]).apply(lambda x:np.nan if len(x[\"adj_analyst_bias\"])<5 else np.mean(x[\"adj_analyst_bias\"]))\n",
    "analyst_avg_bias=analyst_avg_bias.to_frame(name=\"analyst_avg_bias\").reset_index()\n",
    "analyst_avg_bias[\"next_reporting_period\"]=analyst_avg_bias[\"REPORTING_PERIOD\"].apply(lambda x:str(int(x)+10000))\n",
    "del analyst_avg_bias[\"REPORTING_PERIOD\"]\n",
    "\n",
    "df_est[\"ANN_PERIOD\"]=df_est[\"DataDate\"].apply(lambda x:x[0:4]+\"1231\" if x[4:6]>'04' else str(int(x[0:4])-1)+'1231')\n",
    "df_est=pd.merge(df_est,analyst_avg_bias,left_on=[\"ANALYST_NAME\",\"ANN_PERIOD\"],right_on=[\"ANALYST_NAME\",\"next_reporting_period\"],how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#对过去3个月（过去三个月研报数小于5的个股使用过去6月的数据，对于过去六个月没有研报的个股填充nan值）的数据进行过滤，同一分析师对同一个股的多篇研报只取最后一篇\n",
    "#并对过去三个月的一致预期数据按照时间和分析师偏差进行双重加权（时间越近权重越大，分析师偏差越小权重越大）\n",
    "\n",
    "date_calendar=pd.date_range(\"{}-01-01\".format(START_YEAR),\"2020-03-10\")\n",
    "date_calendar=[str(i)[0:4]+str(i)[5:7]+str(i)[8:10] for i in date_calendar]\n",
    "\n",
    "def Process_Single_StkPeriod(df):\n",
    "    date_ls=list(df[\"DataDate\"].drop_duplicates())\n",
    "    df.set_index(\"DataDate\",inplace=True)\n",
    "    df[\"DataDate\"]=df.index\n",
    "    \n",
    "    #按照分析师上一年预测偏差以及研报发布时间进行双重加权\n",
    "    def con_est_calcu(est_data):\n",
    "        if len(est_data) ==0:\n",
    "            return np.nan\n",
    "        analyst_bias=est_data[\"analyst_avg_bias\"]\n",
    "        max_bias=np.nanmax(analyst_bias)\n",
    "        min_bias=np.nanmin(analyst_bias)\n",
    "        if max_bias==min_bias or (math.isnan(min_bias) or math.isnan(max_bias)):\n",
    "            num = len(analyst_bias)\n",
    "            weight_analyst=np.array([1/num]*num)\n",
    "        else:    \n",
    "            weight_analyst=analyst_bias.apply(lambda x:1 if math.isnan(x) else 3-2*(x-min_bias)/(max_bias-min_bias))\n",
    "        weight_analyst=weight_analyst/np.sum(weight_analyst)\n",
    "\n",
    "        est_data[\"date_to_now\"]=est_data[\"DataDate\"].apply(lambda x:date_calendar.index(date)-date_calendar.index(x))\n",
    "        weight_date=np.power(0.5**(1/45),np.array(est_data[\"date_to_now\"]))\n",
    "        weight_date=weight_date/np.sum(weight_date)\n",
    "        \n",
    "        total_weight=weight_analyst*weight_date\n",
    "        total_weight=np.array(total_weight/np.sum(total_weight))\n",
    "        \n",
    "        est_eps=est_data[\"EST_EPS_DILUTED\"].values\n",
    "        median = np.median(est_eps)\n",
    "        MAD =  1.483 * np.median(np.abs(est_eps - median))\n",
    "        est_eps = np.clip(est_eps,median-3*MAD,median+3*MAD)\n",
    "        \n",
    "        if len(est_eps)>=3:\n",
    "            max_index = np.argmax(est_eps)\n",
    "            min_index = np.argmin(est_eps)\n",
    "            est_eps = np.delete(est_eps,[max_index,min_index])\n",
    "            total_weight = np.delete(total_weight,[max_index,min_index])\n",
    "            total_weight = total_weight/np.sum(total_weight)\n",
    "            \n",
    "        if len(est_eps)>=5:\n",
    "            max_index = np.argmax(est_eps)\n",
    "            min_index = np.argmin(est_eps)\n",
    "            est_eps = np.delete(est_eps,[max_index,min_index])\n",
    "            total_weight = np.delete(total_weight,[max_index,min_index])\n",
    "            total_weight = total_weight/np.sum(total_weight)\n",
    "        \n",
    "        con_est_eps=np.sum(np.array(est_eps)*total_weight)\n",
    "\n",
    "        return con_est_eps\n",
    "    \n",
    "    single_stkperiod_result=pd.DataFrame(index=date_ls,columns=[\"con_est_eps\"])\n",
    "    for date in date_ls:\n",
    "        date_range=date_calendar[date_calendar.index(date)-89:date_calendar.index(date)+1]#去过去90天数据（包含今天）\n",
    "        est_data=df.loc[date_range]\n",
    "        est_data=est_data[pd.notnull(est_data[\"EST_EPS_DILUTED\"])]\n",
    "        est_data=est_data.groupby(\"ANALYST_NAME\").apply(lambda x:x.iloc[-1])#同一分析师多篇研报只取最后一篇\n",
    "        \n",
    "        #如果过去90天研报数小于10，则取过去180天研报\n",
    "        if len(est_data)<10:\n",
    "            date_range=date_calendar[date_calendar.index(date)-179:date_calendar.index(date)+1]\n",
    "            est_data=df.loc[date_range]\n",
    "            est_data=est_data[pd.notnull(est_data[\"EST_EPS_DILUTED\"])]\n",
    "            \n",
    "            \"\"\"if (1<=len(est_data)<=3):\n",
    "                con_est_data = est_data[\"EST_EPS_DILUTED\"].iloc[-1]\n",
    "                single_stkperiod_result.loc[date,\"con_est_eps\"]=con_est_data\n",
    "                return single_stkperiod_result\"\"\"\n",
    "            \n",
    "        con_est_data=con_est_calcu(est_data)\n",
    "        single_stkperiod_result.loc[date,\"con_est_eps\"]=con_est_data\n",
    "\n",
    "    return single_stkperiod_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df = df_est[(df_est[\"sid\"]=='600380.SH') & (df_est[\"REPORTING_PERIOD\"] == '20181231')]\n",
    "temp_df=temp_df[temp_df[\"DataDate\"]>'20171110']\n",
    "df = Process_Single_StkPeriod(temp_df)\n",
    "plt.plot(pd.to_datetime(df.index),np.array(df[\"con_est_eps\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_est.groupby(\"sid\").apply(lambda x:x.to_csv(r\"/share/intern_share/analyst_est_data/analyst_est_data_{}.csv\"\\\n",
    "                           .format(x[\"sid\"].iloc[0]),index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped=df_est.groupby([\"sid\",\"REPORTING_PERIOD\"])\n",
    "con_est_eps_df=apply_parallel(grouped,Process_Single_StkPeriod)\n",
    "con_est_eps_df=con_est_eps_df.reset_index().rename(columns={\"level_0\":\"sid\",\"level_1\":\"REPORTING_PERIOD\",\"level_2\":\"DataDate\"})\n",
    "\n",
    "#按照交易日来reindex\n",
    "trade_calendar=get_trade_dates(\"{}0101\".format(START_YEAR+1),\"20200310\")\n",
    "trade_calendar=[str(i) for i in trade_calendar]\n",
    "\n",
    "con_est_eps_df=con_est_eps_df.groupby([\"sid\",\"REPORTING_PERIOD\"],as_index=False)\\\n",
    "                    .apply(lambda x:x.replace(0,np.nan).fillna(method=\"ffill\"))\n",
    "con_est_eps_df=con_est_eps_df.groupby([\"sid\",\"REPORTING_PERIOD\"],as_index=False)\\\n",
    "                    .apply(lambda x:x.drop_duplicates(\"DataDate\").set_index(\"DataDate\").reindex(trade_calendar,method=\"ffill\").dropna().reset_index())\n",
    "\n",
    "con_est_eps_df.to_csv(r\"result/con_eps_my_v3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取企业快报数据\n",
    "sql_1 = \"\"\"SELECT S_INFO_WINDCODE, ANN_DT, REPORT_PERIOD,NET_PROFIT_EXCL_MIN_INT_INC\n",
    "from ASHAREPROFITEXPRESS where ANN_DT between '{}0101' and '20201231'\"\"\".format(START_YEAR)\n",
    "\n",
    "df_ProfitExpress = pd.read_sql_query(sql_1, conn)\n",
    "df_ProfitExpress.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"REPORT_PERIOD\":\"REPORTING_PERIOD\"},inplace=True)\n",
    "\n",
    "#取企业预报数据\n",
    "sql_2 = \"\"\"SELECT S_INFO_WINDCODE, S_PROFITNOTICE_DATE, S_PROFITNOTICE_PERIOD,S_PROFITNOTICE_NETPROFITMIN,S_PROFITNOTICE_NETPROFITMAX\n",
    "from ASHAREPROFITNOTICE where S_PROFITNOTICE_DATE between '{}0101' and '20201231'\"\"\".format(START_YEAR)\n",
    "\n",
    "df_ProfitNotice = pd.read_sql_query(sql_2, conn)\n",
    "df_ProfitNotice.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"S_PROFITNOTICE_PERIOD\":\"REPORTING_PERIOD\"},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##如果企业发布了业绩预报或者快报，则直接采取业绩预报或者快报的数据\n",
    "df_FA_EPS.set_index([\"sid\",\"REPORTING_PERIOD\"],inplace=True)\n",
    "\n",
    "def Fill_with_Notice(df):\n",
    "    ann_date=df[\"S_PROFITNOTICE_DATE\"][df.index[0]]\n",
    "    sid=df[\"sid\"][df.index[0]]\n",
    "    totshare = df_now_tot_share.set_index('sid').loc[sid,'TOT_SHR']\n",
    "    min_profit=df[\"S_PROFITNOTICE_NETPROFITMIN\"][df.index[0]]\n",
    "    max_profit=df[\"S_PROFITNOTICE_NETPROFITMAX\"][df.index[0]]\n",
    "    notice_eps=(min_profit+max_profit)/2/totshare\n",
    "    try:\n",
    "        df[\"con_est_eps\"][df[\"DataDate\"]>=date_calendar[date_calendar.index(ann_date)-1]]=notice_eps\n",
    "    except ValueError as e:\n",
    "        pass\n",
    "    return df\n",
    "    \n",
    "con_est_eps_df_merged=pd.merge(con_est_eps_df,df_ProfitNotice,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "con_est_eps_df_merged=con_est_eps_df_merged.groupby([\"sid\",\"REPORTING_PERIOD\"]).apply(Fill_with_Notice)\n",
    "\n",
    "\n",
    "def Fill_with_Express(df):\n",
    "    sid=df[\"sid\"][df.index[0]]\n",
    "    ann_date=df[\"ANN_DT\"][df.index[0]]\n",
    "    totshare = df_now_tot_share.set_index('sid').loc[sid,'TOT_SHR']\n",
    "    express_eps=df[\"NET_PROFIT_EXCL_MIN_INT_INC\"][df.index[0]]/totshare/10000\n",
    "    try:\n",
    "        df[\"con_est_eps\"][df[\"DataDate\"]>=date_calendar[date_calendar.index(ann_date)-1]]=express_eps\n",
    "    except ValueError as e:\n",
    "        pass\n",
    "    return df\n",
    "    \n",
    "con_est_eps_df_merged=pd.merge(con_est_eps_df_merged,df_ProfitExpress,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "con_est_eps_df_merged=con_est_eps_df_merged.groupby([\"sid\",\"REPORTING_PERIOD\"]).apply(Fill_with_Express)\n",
    "con_est_eps_df_merged=con_est_eps_df_merged[[\"sid\",\"REPORTING_PERIOD\",\"DataDate\",\"con_est_eps\"]]\n",
    "\n",
    "#在企业发布正式年报之后，使用正式年报数据代替一致预期值\n",
    "def Fill_with_FA(df):\n",
    "    sid=df[\"sid\"][df.index[0]]\n",
    "    ann_date=df[\"ANN_DT\"][df.index[0]]\n",
    "    totshare = df_now_tot_share.set_index('sid').loc[sid,'TOT_SHR']\n",
    "    FA_eps=(df['S_FA_DEDUCTEDPROFIT']+df['S_FA_EXTRAORDINARY'])[df.index[0]]/totshare/10000\n",
    "    try:\n",
    "        df[\"con_est_eps\"][df[\"DataDate\"]>=date_calendar[date_calendar.index(ann_date)-1]]=FA_eps\n",
    "    except ValueError as e:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "df_FA_EPS.reset_index(inplace=True)\n",
    "con_est_eps_df_merged=pd.merge(con_est_eps_df_merged,df_FA_EPS,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "con_est_eps_df_merged=con_est_eps_df_merged.groupby([\"sid\",\"REPORTING_PERIOD\"]).apply(Fill_with_FA)\n",
    "\n",
    "#储存数据\n",
    "con_est_eps_df_merged=con_est_eps_df_merged.groupby([\"sid\",\"REPORTING_PERIOD\"],as_index=False)\\\n",
    "                    .apply(lambda x:x.replace(0,np.nan).fillna(method=\"ffill\"))\n",
    "con_est_eps_df_merged=con_est_eps_df_merged[[\"sid\",\"REPORTING_PERIOD\",\"DataDate\",\"con_est_eps\"]]\n",
    "con_est_eps_df_merged.to_csv(r\"result/con_eps_my_v3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取出hs300的数据\n",
    "def get_hs300_data(df):\n",
    "    date=df[\"DataDate\"][df.index[0]]\n",
    "    w_hs300=query_table(\"DailyBar\",start_date=date,end_date=date,fields=[\"w_hs300\",\"w_cs500\"])\n",
    "    hs300_stk_ls=w_hs300[\"sid\"][w_hs300[\"w_hs300\"]!=0].values\n",
    "    df=df.set_index(\"sid\").loc[hs300_stk_ls].reset_index()\n",
    "    return df\n",
    "\n",
    "grouped=con_est_eps_df_merged.groupby(\"DataDate\")\n",
    "hs300_con_est_eps=apply_parallel(grouped,get_hs300_data)\n",
    "hs300_con_est_eps=hs300_con_est_eps.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#筛选出ROE、净利润增速、销售收入增速稳健、市值较大的股票\n",
    "#size:前10%--> ROE：前35%--> growth：前35%\n",
    "\n",
    "df_Barra_Factor=pd.read_csv(r\"/home/ywang/proj_1/data/Asset_Exposure_Data_Calculated.csv\",dtype={'DataDate':str})\n",
    "df_Barra_Factor=pd.merge(df_Barra_Factor,indus_size_df,on=[\"sid\",\"DataDate\"],how='left')\n",
    "\n",
    "sql=\"\"\"select S_INFO_WINDCODE ,S_INFO_NAME,S_INFO_COMPNAME,S_INFO_LISTBOARD FROM ASHAREDESCRIPTION \"\"\"\n",
    "descrip_df=pd.read_sql(sql,conn)\n",
    "descrip_df.rename(columns={'S_INFO_WINDCODE':'sid'},inplace=True)\n",
    "df_Barra_Factor=pd.merge(df_Barra_Factor,descrip_df[['sid','S_INFO_COMPNAME']],on='sid')\n",
    "\n",
    "def Stock_Filter(df):\n",
    "    df=df.sort_values(\"mktcap\",ascending=False).iloc[0:15]\n",
    "    \n",
    "    df[\"DataDate\"]=df[\"DataDate\"].apply(str)\n",
    "    return df[[\"sid\",\"DataDate\",\"S_INFO_COMPNAME\",\"BarraETOP\",\"BarraSGRO\",\"BarraEGRO\",\"BarraEGIBS\",\"BarraEGIBS_s\"]]\n",
    "\n",
    "grouped=df_Barra_Factor.groupby([\"DataDate\",\"L1_INDUSTRY\"])\n",
    "df_Stock_Pool=apply_parallel(grouped,Stock_Filter)\n",
    "StockPool_con_est_eps=pd.merge(con_est_eps_df_final,df_Stock_Pool,on=[\"sid\",\"DataDate\"])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_PETTM=pd.read_csv(r\"data/PETTM.csv\",dtype={\"DataDate\":str})\\n\\ndef Rolling_Median(df):\\n    df[\"pe_2y_rollingmedian\"]=df[\"S_VAL_PE\"].rolling(252*2).apply(np.median)\\n    return df\\n\\ngrouped=df_PETTM.groupby(\"sid\")\\ndf_PETTM_2y_rollingmedian=apply_parallel(grouped,Rolling_Median)\\nhs300_con_est_eps=pd.merge(hs300_con_est_eps,df_PETTM_2y_rollingmedian,on=[\"sid\",\"DataDate\"],how=\"left\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加入过去两年PE_TTM的 rolling_median 值（相较于mean值更为稳定，不易受极端值影响）\n",
    "\n",
    "sql = \"\"\"SELECT S_INFO_WINDCODE, TRADE_DT, S_VAL_PE_TTM,S_DQ_MV,S_VAL_PB_NEW from ASHAREEODDERIVATIVEINDICATOR\n",
    "where TRADE_DT between '{}0101' and '20201231'\"\"\".format(START_YEAR-1)  \n",
    "df_PETTM = pd.read_sql_query(sql, conn)\n",
    "df_PETTM=df_PETTM.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"TRADE_DT\":\"DataDate\"})\n",
    "df_PETTM=df_PETTM.sort_values([\"sid\",\"DataDate\"])\n",
    "df_PETTM.to_csv(r\"data/PETTM.csv\",index=False)\n",
    "'''\n",
    "df_PETTM=pd.read_csv(r\"data/PETTM.csv\",dtype={\"DataDate\":str})\n",
    "\n",
    "def Rolling_Median(df):\n",
    "    df[\"pe_2y_rollingmedian\"]=df[\"S_VAL_PE\"].rolling(252*2).apply(np.median)\n",
    "    return df\n",
    "\n",
    "grouped=df_PETTM.groupby(\"sid\")\n",
    "df_PETTM_2y_rollingmedian=apply_parallel(grouped,Rolling_Median)\n",
    "hs300_con_est_eps=pd.merge(hs300_con_est_eps,df_PETTM_2y_rollingmedian,on=[\"sid\",\"DataDate\"],how=\"left\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加入每天的收盘价序列\n",
    "\n",
    "sql = \"\"\"SELECT S_INFO_WINDCODE, TRADE_DT, S_DQ_ADJCLOSE,S_DQ_ADJFACTOR  from ASHAREEODPRICES \n",
    "where TRADE_DT between '{}0101' and '20201231'\"\"\".format(START_YEAR+1)    \n",
    "df_close_hfq = pd.read_sql_query(sql, conn)\n",
    "df_close_hfq=df_close_hfq.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"TRADE_DT\":\"DataDate\"})\n",
    "df_close_hfq=df_close_hfq.sort_values([\"sid\",\"DataDate\"])\n",
    "def fun(df):\n",
    "    df[\"S_DQ_ADJCLOSE\"]=df[\"S_DQ_ADJCLOSE\"]/df[\"S_DQ_ADJFACTOR\"].iloc[-1]\n",
    "    return df\n",
    "df_close_qfq=df_close_hfq.groupby(\"sid\").apply(fun)\n",
    "df_close_qfq.to_csv(r\"data/close_price.csv\",index=False)\n",
    "'''\n",
    "df_close=pd.read_csv(r\"data/close_price.csv\",dtype={\"DataDate\":str})\n",
    "\n",
    "hs300_con_est_eps=pd.merge(hs300_con_est_eps,df_close,on=[\"sid\",\"DataDate\"],how=\"left\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
