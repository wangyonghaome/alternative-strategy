{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_tools.api import *\n",
    "from utilscht.Data import *\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from joblib import Parallel,delayed\n",
    "import tushare as ts\n",
    "import datetime\n",
    "import math\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "START_YEAR=2014\n",
    "\n",
    "DB_INFO = dict(host='192.168.1.234',\n",
    "               user='winduser',\n",
    "               password='1qaz@WSX',\n",
    "               db='wind')\n",
    "\n",
    "conn = pymysql.connect(**DB_INFO, charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_parallel(df_grouped, func, n_jobs=16, backend='loky', as_index=True, **kwargs):\n",
    "    \"\"\"\n",
    "    This is for parallel between grouped generated by pd.DataFrame.groupby\n",
    "    :param df_grouped:\n",
    "    :param func:\n",
    "    :param n_jobs:\n",
    "    :param backend:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    names = []\n",
    "    groups = []\n",
    "    for name, group in df_grouped:\n",
    "        names.append(name)\n",
    "        groups.append(group)\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=5, backend=backend, batch_size='auto') \\\n",
    "        (delayed(func)(group, **kwargs) for group in groups)\n",
    "\n",
    "    return pd.concat(results, keys=names if as_index else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 下面三个单元格分别是取全部股票的数据、对全部数据进行处理以获得分析师在每年的主动偏差、分股票储存数据，运行需要的时间较长，故已经预先运行好，将结果保存在/share/intern_share/analyst_est_data/. 下面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取计算所需数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#获取行业和市值\n",
    "indus_size_df=query_table(\"DailyBar\",start_date=\"{}0101\".format(START_YEAR),end_date=\"20201231\",fields=[\"L1_INDUSTRY\",\"mktcap\"])\n",
    "indus_size_df[\"DataDate\"]=indus_size_df[\"DataDate\"].apply(lambda x:str(x))\n",
    "\n",
    "#获取研报预期数据\n",
    "sql = \"\"\"SELECT S_INFO_WINDCODE, EST_DT, REPORTING_PERIOD,RESEARCH_INST_NAME, ANALYST_NAME,EST_NET_PROFIT \n",
    "from ASHAREEARNINGEST where EST_DT between '{}0101' and '20201231'\"\"\".format(START_YEAR)   \n",
    "df_est = pd.read_sql_query(sql, conn)\n",
    "df_est.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"EST_DT\":\"DataDate\"},inplace=True)\n",
    "df_est=df_est.sort_values([\"sid\",\"DataDate\",\"REPORTING_PERIOD\"])\n",
    "df_est=pd.merge(df_est,indus_size_df,on=[\"sid\",\"DataDate\"],how=\"left\")\n",
    "\n",
    "#获取发布的年报数据\n",
    "sql = \"\"\"SELECT S_INFO_WINDCODE, ANN_DT, REPORT_PERIOD,S_FA_DEDUCTEDPROFIT,S_FA_EXTRAORDINARY\n",
    "from ASHAREFINANCIALINDICATOR where ANN_DT between '{}0101' and '20201231'\"\"\".format(START_YEAR)\n",
    "df_FA_EPS = pd.read_sql_query(sql, conn)\n",
    "df_FA_EPS.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"REPORT_PERIOD\":\"REPORTING_PERIOD\"},inplace=True)\n",
    "df_est=pd.merge(df_est,df_FA_EPS,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "\n",
    "#获取总股本信息\n",
    "sql=\"select S_INFO_WINDCODE,TOT_SHR,CHANGE_DT from AShareCapitalization\".upper()\n",
    "df_tot_share=pd.read_sql_query(sql,conn)\n",
    "df_tot_share.rename(columns={\"S_INFO_WINDCODE\":\"sid\"},inplace=True)\n",
    "df_tot_share=df_tot_share.sort_values([\"sid\",\"CHANGE_DT\"])\n",
    "df_now_tot_share=df_tot_share.groupby('sid',as_index=False).apply(lambda x:x.iloc[-1])\n",
    "\n",
    "#计算前复权每股预期收益\n",
    "df_est=pd.merge(df_est,df_now_tot_share,on='sid',how='left')\n",
    "df_est['EST_EPS_DILUTED']=df_est['EST_NET_PROFIT']/df_est['TOT_SHR']\n",
    "df_est['S_FA_EPS_DILUTED']=(df_est['S_FA_DEDUCTEDPROFIT']+df_est['S_FA_EXTRAORDINARY'])/df_est['TOT_SHR']/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算分析师的主观偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算距离年末的月数（第二年初的预测月数为负值）\n",
    "df_1y_est=df_est.groupby([\"sid\",\"DataDate\",\"ANALYST_NAME\"],as_index=False).apply(lambda x:x.iloc[0]).reset_index(drop=True)\n",
    "df_1y_est[\"Date_to_Reporting\"]=df_1y_est[[\"DataDate\",\"REPORTING_PERIOD\"]].apply(lambda x:int(x[1][4:6])-int(x[0][4:6]) if x[0][0:4]==x[1][0:4] else int(x[1][4:6])-int(x[0][4:6])-12,axis=1)\n",
    "\n",
    "#计算偏差比例\n",
    "df_1y_est[\"Analyst_Bias\"]=np.abs(df_1y_est[\"S_FA_EPS_DILUTED\"]-df_1y_est[\"EST_EPS_DILUTED\"])/df_1y_est[\"S_FA_EPS_DILUTED\"]\n",
    "\n",
    "#按照年度将偏差对行业、市值、距离年末月数进行回归取残差\n",
    "def Regress_by_Year(df):\n",
    "    df=df[pd.notnull(df[\"Analyst_Bias\"])&pd.notnull(df[\"L1_INDUSTRY\"])]\n",
    "    if len(df)==0:\n",
    "        return df\n",
    "    bias=np.array(df[\"Analyst_Bias\"])\n",
    "    \n",
    "    indus_dummy=np.array(pd.get_dummies(df[\"L1_INDUSTRY\"]))\n",
    "    mktv=np.array(df[\"mktcap\"])\n",
    "    date_to_reporting=np.array(df[\"Date_to_Reporting\"])\n",
    "    \n",
    "    model=sm.OLS(bias,np.column_stack([indus_dummy,mktv,date_to_reporting]))\n",
    "    result=model.fit()\n",
    "    df[\"adj_analyst_bias\"]=result.resid\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_1y_est[\"adj_analyst_bias\"]=np.nan\n",
    "df_1y_est=df_1y_est.groupby(\"REPORTING_PERIOD\",as_index=False).apply(lambda x:Regress_by_Year(x)).reset_index(drop=True)\n",
    "\n",
    "#按照预测年度、分析师来计算偏差平均值作为下一年度该分析师的可信水平\n",
    "analyst_avg_bias=df_1y_est.groupby([\"REPORTING_PERIOD\",\"ANALYST_NAME\"]).apply(lambda x:np.nan if len(x[\"adj_analyst_bias\"])<5 else np.mean(x[\"adj_analyst_bias\"]))\n",
    "analyst_avg_bias=analyst_avg_bias.to_frame(name=\"analyst_avg_bias\").reset_index()\n",
    "analyst_avg_bias[\"next_reporting_period\"]=analyst_avg_bias[\"REPORTING_PERIOD\"].apply(lambda x:str(int(x)+10000))\n",
    "del analyst_avg_bias[\"REPORTING_PERIOD\"]\n",
    "\n",
    "df_est[\"ANN_PERIOD\"]=df_est[\"DataDate\"].apply(lambda x:x[0:4]+\"1231\" if x[4:6]>'04' else str(int(x[0:4])-1)+'1231')\n",
    "df_est=pd.merge(df_est,analyst_avg_bias,left_on=[\"ANALYST_NAME\",\"ANN_PERIOD\"],right_on=[\"ANALYST_NAME\",\"next_reporting_period\"],how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将数据按股票代码存到文件夹下面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_est.groupby(\"sid\").apply(lambda x:x.to_csv(r\"/share/intern_share/analyst_est_data/analyst_est_data_{}.csv\"\\\n",
    "                           .format(x[\"sid\"].iloc[0]),index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 下面是针对每一只股票进行的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算个股一致预期盈利数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#对过去3个月（过去三个月研报数小于5的个股使用过去6月的数据，对于过去六个月没有研报的个股填充nan值）的数据进行过滤，同一分析师对同一个股的多篇研报只取最后一篇\n",
    "#并对过去三个月的一致预期数据按照时间和分析师偏差进行双重加权（时间越近权重越大，分析师偏差越小权重越大）\n",
    "\n",
    "date_calendar=pd.date_range(\"{}-01-01\".format(START_YEAR-1),\"2020-03-10\")\n",
    "date_calendar=[str(i)[0:4]+str(i)[5:7]+str(i)[8:10] for i in date_calendar]\n",
    "\n",
    "#按照分析师上一年预测偏差以及研报发布时间进行双重加权\n",
    "def con_est_calcu(est_data,date,date_calendar):\n",
    "    if len(est_data) ==0:\n",
    "        return np.nan\n",
    "    analyst_bias=est_data[\"analyst_avg_bias\"]\n",
    "    max_bias=np.nanmax(analyst_bias)\n",
    "    min_bias=np.nanmin(analyst_bias)\n",
    "    #通过上一年度分析师偏差计算分析师维度上的权重，参数可进行调整\n",
    "    if max_bias==min_bias or (math.isnan(min_bias) or math.isnan(max_bias)):\n",
    "        num = len(analyst_bias)\n",
    "        weight_analyst=np.array([1/num]*num)\n",
    "    else:    \n",
    "        weight_analyst=analyst_bias.apply(lambda x:1 if math.isnan(x) else 3-2*(x-min_bias)/(max_bias-min_bias))\n",
    "    weight_analyst=weight_analyst/np.sum(weight_analyst)\n",
    "\n",
    "    #确定时间维度上的权重，45天为半衰期，参数可进行调整\n",
    "    est_data[\"date_to_now\"]=est_data[\"DataDate\"].apply(lambda x:date_calendar.index(date)-date_calendar.index(x))\n",
    "    weight_date=np.power(0.5**(1/45),np.array(est_data[\"date_to_now\"]))\n",
    "    weight_date=weight_date/np.sum(weight_date)\n",
    "\n",
    "    total_weight=weight_analyst*weight_date\n",
    "    total_weight=np.array(total_weight/np.sum(total_weight))\n",
    "\n",
    "    #median_mad法去除极端值\n",
    "    est_eps=est_data[\"EST_EPS_DILUTED\"].values\n",
    "    median = np.median(est_eps)\n",
    "    MAD =  1.483 * np.median(np.abs(est_eps - median))\n",
    "    est_eps = np.clip(est_eps,median-3*MAD,median+3*MAD)\n",
    "\n",
    "    #若剩余研报数大于等于3，则去掉一个最高值和最低值\n",
    "    if len(est_eps)>=3:\n",
    "        max_index = np.argmax(est_eps)\n",
    "        min_index = np.argmin(est_eps)\n",
    "        est_eps = np.delete(est_eps,[max_index,min_index])\n",
    "        total_weight = np.delete(total_weight,[max_index,min_index])\n",
    "        total_weight = total_weight/np.sum(total_weight)\n",
    "\n",
    "    con_est_eps=np.sum(np.array(est_eps)*total_weight)\n",
    "\n",
    "    return con_est_eps\n",
    "\n",
    "def Process_Single_StkPeriod(df):\n",
    "    date_ls=list(set(df.index))\n",
    "    date_ls.sort()\n",
    "    \n",
    "    single_stkperiod_result=pd.DataFrame(index=date_ls,columns=[\"con_est_eps\"])\n",
    "    for date in date_ls:\n",
    "        date_range=date_calendar[date_calendar.index(date)-89:date_calendar.index(date)+1]#取过去90天数据（包含今天）\n",
    "        est_data=df.loc[date_range]\n",
    "        est_data=est_data[pd.notnull(est_data[\"EST_EPS_DILUTED\"])]\n",
    "        est_data=est_data.groupby(\"ANALYST_NAME\").apply(lambda x:x.iloc[-1])#同一分析师多篇研报只取最后一篇\n",
    "        \n",
    "        #如果过去90天研报数小于10，则取过去180天研报，参数可进行调整\n",
    "        if len(est_data)<10:\n",
    "            date_range=date_calendar[date_calendar.index(date)-179:date_calendar.index(date)+1]\n",
    "            est_data=df.loc[date_range]\n",
    "            est_data=est_data[pd.notnull(est_data[\"EST_EPS_DILUTED\"])]\n",
    "            \n",
    "        con_est_data=con_est_calcu(est_data,date,date_calendar)\n",
    "        single_stkperiod_result.loc[date,\"con_est_eps\"]=con_est_data\n",
    "\n",
    "    return single_stkperiod_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_con_est_eps(sid):\n",
    "    df_est = pd.read_csv(r\"/share/intern_share/analyst_est_data/analyst_est_data_{}.csv\".format(sid),\n",
    "                        dtype = {\"DataDate\":str,\"REPORTING_PERIOD\":str})\n",
    "    df_est = df_est.set_index(\"DataDate\")\n",
    "    df_est[\"DataDate\"] = df_est.index\n",
    "    con_est_eps_df =df_est.groupby([\"sid\",\"REPORTING_PERIOD\"]).apply(Process_Single_StkPeriod)\n",
    "    con_est_eps_df=con_est_eps_df.reset_index().rename(columns={\"level_0\":\"sid\",\"level_1\":\"REPORTING_PERIOD\",\"level_2\":\"DataDate\"})\n",
    "    #按照交易日来reindex\n",
    "    trade_calendar=get_trade_dates(\"{}0101\".format(START_YEAR),\"20200310\")\n",
    "    trade_calendar=[str(i) for i in trade_calendar]\n",
    "\n",
    "    con_est_eps_df=con_est_eps_df.groupby([\"sid\",\"REPORTING_PERIOD\"],as_index=False)\\\n",
    "                        .apply(lambda x:x.replace(0,np.nan).fillna(method=\"ffill\"))\n",
    "    con_est_eps_df=con_est_eps_df.groupby([\"sid\",\"REPORTING_PERIOD\"],as_index=False)\\\n",
    "                        .apply(lambda x:x.drop_duplicates(\"DataDate\").set_index(\"DataDate\").reindex(trade_calendar,method=\"ffill\").dropna().reset_index())\n",
    "    con_est_eps_df = con_est_eps_df.reset_index(drop=True)\n",
    "    return con_est_eps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对一致预期值进行调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjust_data(sid,conn):\n",
    "    #取企业快报数据\n",
    "    sql_1 = \"\"\"SELECT S_INFO_WINDCODE, ANN_DT, REPORT_PERIOD,NET_PROFIT_EXCL_MIN_INT_INC\n",
    "    from ASHAREPROFITEXPRESS where S_INFO_WINDCODE = '{}' and ANN_DT between '{}0101' and '20201231'\"\"\".format(sid,START_YEAR)\n",
    "\n",
    "    df_ProfitExpress = pd.read_sql_query(sql_1, conn)\n",
    "    df_ProfitExpress.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"REPORT_PERIOD\":\"REPORTING_PERIOD\"},inplace=True)\n",
    "\n",
    "    #取企业预报数据\n",
    "    sql_2 = \"\"\"SELECT S_INFO_WINDCODE, S_PROFITNOTICE_DATE, S_PROFITNOTICE_PERIOD,S_PROFITNOTICE_NETPROFITMIN,S_PROFITNOTICE_NETPROFITMAX\n",
    "    from ASHAREPROFITNOTICE where S_INFO_WINDCODE = '{}' and S_PROFITNOTICE_DATE between '{}0101' and '20201231'\"\"\".format(sid,START_YEAR)\n",
    "\n",
    "    df_ProfitNotice = pd.read_sql_query(sql_2, conn)\n",
    "    df_ProfitNotice.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"S_PROFITNOTICE_PERIOD\":\"REPORTING_PERIOD\"},inplace=True)\n",
    "\n",
    "    #获取发布的年报数据\n",
    "    sql = \"\"\"SELECT S_INFO_WINDCODE, ANN_DT, REPORT_PERIOD,S_FA_DEDUCTEDPROFIT,S_FA_EXTRAORDINARY\n",
    "    from ASHAREFINANCIALINDICATOR where S_INFO_WINDCODE = '{}' and ANN_DT between '{}0101' and '20201231'\"\"\".format(sid,START_YEAR)\n",
    "    df_FA_EPS = pd.read_sql_query(sql, conn)\n",
    "    df_FA_EPS.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"REPORT_PERIOD\":\"REPORTING_PERIOD\"},inplace=True)\n",
    "\n",
    "    #获取总股本信息\n",
    "    sql=\"select S_INFO_WINDCODE,TOT_SHR,CHANGE_DT from AShareCapitalization where S_INFO_WINDCODE = '{}' \".format(sid).upper()\n",
    "    df_tot_share=pd.read_sql_query(sql,conn)\n",
    "    df_tot_share.rename(columns={\"S_INFO_WINDCODE\":\"sid\"},inplace=True)\n",
    "    df_tot_share=df_tot_share.sort_values([\"sid\",\"CHANGE_DT\"])\n",
    "    df_now_tot_share=df_tot_share.groupby('sid',as_index=False).apply(lambda x:x.iloc[-1])\n",
    "    \n",
    "    return df_ProfitExpress,df_ProfitNotice,df_FA_EPS,df_now_tot_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjusted_con_est_eps(sid,conn,file_path=\"result//\"): \n",
    "    \"\"\"\n",
    "    params:\n",
    "    sid: the stock code, 000001.SZ for example\n",
    "    conn: the sql connection\n",
    "    file_path: the path to store the result\n",
    "\n",
    "    function: \n",
    "    generate and store the adjusted consensus estimate eps file\n",
    "    \"\"\"\n",
    "    \n",
    "    ##如果企业发布了业绩预报或者快报，则直接采取业绩预报或者快报的数据\n",
    "    con_est_eps_df = get_con_est_eps(sid)\n",
    "    df_ProfitExpress,df_ProfitNotice,df_FA_EPS,df_now_tot_share =get_adjust_data(sid,conn)\n",
    "    df_FA_EPS.set_index([\"sid\",\"REPORTING_PERIOD\"],inplace=True)\n",
    "\n",
    "    def Fill_with_Notice(df):\n",
    "        ann_date=df[\"S_PROFITNOTICE_DATE\"][df.index[0]]\n",
    "        sid=df[\"sid\"][df.index[0]]\n",
    "        totshare = df_now_tot_share.set_index('sid').loc[sid,'TOT_SHR']\n",
    "        min_profit=df[\"S_PROFITNOTICE_NETPROFITMIN\"][df.index[0]]\n",
    "        max_profit=df[\"S_PROFITNOTICE_NETPROFITMAX\"][df.index[0]]\n",
    "        try:\n",
    "            notice_eps=(min_profit+max_profit)/2/totshare\n",
    "            df[\"con_est_eps\"][df[\"DataDate\"]>=date_calendar[date_calendar.index(ann_date)-1]]=notice_eps\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "        except TypeError as e:\n",
    "            pass\n",
    "        return df\n",
    "\n",
    "    con_est_eps_df_merged=pd.merge(con_est_eps_df,df_ProfitNotice,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "    con_est_eps_df_merged=con_est_eps_df_merged.groupby([\"sid\",\"REPORTING_PERIOD\"]).apply(Fill_with_Notice)\n",
    "\n",
    "\n",
    "    def Fill_with_Express(df):\n",
    "        sid=df[\"sid\"][df.index[0]]\n",
    "        ann_date=df[\"ANN_DT\"][df.index[0]]\n",
    "        totshare = df_now_tot_share.set_index('sid').loc[sid,'TOT_SHR']\n",
    "        express_eps=df[\"NET_PROFIT_EXCL_MIN_INT_INC\"][df.index[0]]/totshare/10000\n",
    "        try:\n",
    "            df[\"con_est_eps\"][df[\"DataDate\"]>=date_calendar[date_calendar.index(ann_date)-1]]=express_eps\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "        return df\n",
    "\n",
    "    con_est_eps_df_merged=pd.merge(con_est_eps_df_merged,df_ProfitExpress,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "    con_est_eps_df_merged=con_est_eps_df_merged.groupby([\"sid\",\"REPORTING_PERIOD\"]).apply(Fill_with_Express)\n",
    "    con_est_eps_df_merged=con_est_eps_df_merged[[\"sid\",\"REPORTING_PERIOD\",\"DataDate\",\"con_est_eps\"]]\n",
    "\n",
    "    #在企业发布正式年报之后，使用正式年报数据代替一致预期值\n",
    "    def Fill_with_FA(df):\n",
    "        sid=df[\"sid\"][df.index[0]]\n",
    "        ann_date=df[\"ANN_DT\"][df.index[0]]\n",
    "        totshare = df_now_tot_share.set_index('sid').loc[sid,'TOT_SHR']\n",
    "        FA_eps=(df['S_FA_DEDUCTEDPROFIT']+df['S_FA_EXTRAORDINARY'])[df.index[0]]/totshare/10000\n",
    "        try:\n",
    "            df[\"con_est_eps\"][df[\"DataDate\"]>=date_calendar[date_calendar.index(ann_date)-1]]=FA_eps\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "        return df\n",
    "\n",
    "    df_FA_EPS.reset_index(inplace=True)\n",
    "    con_est_eps_df_merged=pd.merge(con_est_eps_df_merged,df_FA_EPS,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "    con_est_eps_df_merged=con_est_eps_df_merged.groupby([\"sid\",\"REPORTING_PERIOD\"]).apply(Fill_with_FA)\n",
    "\n",
    "    #储存数据\n",
    "    con_est_eps_df_merged=con_est_eps_df_merged.groupby([\"sid\",\"REPORTING_PERIOD\"],as_index=False)\\\n",
    "                        .apply(lambda x:x.replace(0,np.nan).fillna(method=\"ffill\"))\n",
    "    con_est_eps_df_merged=con_est_eps_df_merged[[\"sid\",\"REPORTING_PERIOD\",\"DataDate\",\"con_est_eps\"]]\n",
    "    con_est_eps_df_merged.to_csv(file_path+\"con_eps_my_{}.csv\".format(sid),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取一致预期值、股票收盘价、PE值并merge到一个dataframe中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_eps_pe_price(sid,conn):\n",
    "    #获取一致预期数据\n",
    "    con_expect_data=pd.read_csv(r\"result/con_eps_my_{}.csv\".format(sid),dtype={\"DataDate\":str,\"REPORTING_PERIOD\":str})\n",
    "    con_expect_data=con_expect_data.groupby([\"sid\",\"REPORTING_PERIOD\"],as_index=False)\\\n",
    "                        .apply(lambda x:x.replace(0,np.nan).fillna(method=\"ffill\"))\n",
    "    open_price_adj=pd.read_csv(r\"data/open_price.csv\",dtype={\"DataDate\":str})\n",
    "    con_expect_data=con_expect_data[[\"sid\",\"REPORTING_PERIOD\",\"DataDate\",\"con_est_eps\"]]\n",
    "\n",
    "    stock_pool_est_eps=con_expect_data.set_index(\"sid\")\n",
    "    stock_pool_est_eps=stock_pool_est_eps[stock_pool_est_eps[\"DataDate\"]>\"{}1101\".format(START_YEAR)].reset_index()\n",
    "\n",
    "    #加入收盘价序列\n",
    "    sql = \"\"\"SELECT S_INFO_WINDCODE, TRADE_DT, S_DQ_ADJCLOSE,S_DQ_ADJFACTOR  from ASHAREEODPRICES \n",
    "    where S_INFO_WINDCODE = '{}' and  TRADE_DT between '{}0101' and '20201231'\"\"\".format(sid,START_YEAR+1)    \n",
    "    df_close_hfq = pd.read_sql_query(sql, conn)\n",
    "    df_close_hfq=df_close_hfq.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"TRADE_DT\":\"DataDate\"})\n",
    "    df_close_hfq=df_close_hfq.sort_values([\"sid\",\"DataDate\"])\n",
    "    df_close_hfq[\"S_DQ_ADJCLOSE\"]=df_close_hfq[\"S_DQ_ADJCLOSE\"]/df_close_hfq[\"S_DQ_ADJFACTOR\"].iloc[-1]\n",
    "\n",
    "    stock_pool_est_eps=pd.merge(stock_pool_est_eps,df_close_hfq,on=[\"sid\",\"DataDate\"],how=\"left\")\n",
    "\n",
    "    #利用过去两年 PE中位数作为当前合理的 PE\n",
    "    sql = \"\"\"SELECT S_INFO_WINDCODE, TRADE_DT, S_VAL_PE_TTM,S_DQ_MV,S_VAL_PB_NEW from ASHAREEODDERIVATIVEINDICATOR\n",
    "    where S_INFO_WINDCODE = '{}' and TRADE_DT between '{}0101' and '20201231'\"\"\".format(sid,START_YEAR-1)  \n",
    "    df_PETTM = pd.read_sql_query(sql, conn)\n",
    "    df_PETTM=df_PETTM.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"TRADE_DT\":\"DataDate\"})\n",
    "    df_PETTM=df_PETTM.sort_values([\"sid\",\"DataDate\"])\n",
    "    \n",
    "    #参数可进行调整\n",
    "    def Rolling_Median(df):\n",
    "        df[\"S_VAL_PE_TTM\"]=df[\"S_VAL_PE_TTM\"].fillna(method='bfill')\n",
    "        df[\"pe_2y_rollingmedian\"]=df[\"S_VAL_PE_TTM\"].rolling(252*3).apply(lambda x:np.percentile(x,40)) #取滚动40%分位数\n",
    "        return df\n",
    "\n",
    "    df_PETTM_2y_rollingmedian=Rolling_Median(df_PETTM)\n",
    "    stock_pool_est_eps=pd.merge(stock_pool_est_eps,df_PETTM_2y_rollingmedian,on=[\"sid\",\"DataDate\"],how=\"left\")\n",
    "    return stock_pool_est_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行画图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse the perfromance of the strategy on single stock\n",
    "def get_prices_v1(daily_df):\n",
    "    close=daily_df[\"S_DQ_ADJCLOSE\"].values[0]\n",
    "    date=daily_df[\"DataDate\"].values[0]\n",
    "\n",
    "    prices=np.array([0,0,0,0],dtype=float)\n",
    "    for i in range(4):\n",
    "        try:\n",
    "            reporting_period=str(int(date[0:4])+i-1)+\"1231\"\n",
    "            con_est_eps=daily_df[daily_df[\"REPORTING_PERIOD\"]==reporting_period][\"con_est_eps\"].values[0]\n",
    "            pe=daily_df[daily_df[\"REPORTING_PERIOD\"]==reporting_period][\"pe_2y_rollingmedian\"].values[0]\n",
    "            prices[i]=con_est_eps*pe\n",
    "        except IndexError as e:\n",
    "            pe = np.nan\n",
    "            pass\n",
    "        \n",
    "    #若第三年con_eps 为空，则线性填充第三年数据\n",
    "    if prices[2]==0:\n",
    "        prices[2]= prices[1]+(prices[1]-prices[0])\n",
    "    \n",
    "    #处理三年con_eps 不随时间递增的情况 (p0, p1 可能因为一大笔非常规性收益而异常过大)\n",
    "    if prices[0]>prices[1] and prices[1]<prices[2]:\n",
    "        prices[0] = prices[1]-(prices[2]-prices[1]) \n",
    "    elif prices[1]>prices[2] and prices[0]<prices[2]:\n",
    "        prices[1] = (prices[0]+prices[2])/2\n",
    "    \n",
    "    #若第四年con_eps 为空，则线性填充第四年数据\n",
    "    if prices[3] ==0:\n",
    "        prices[3]=prices[2]+(prices[2]-prices[1])\n",
    "    \n",
    "        \n",
    "    #进行平滑（向未来一年进行滚动）\n",
    "    month , day =( int(date[4:6]), int(date[6:8]) )\n",
    "    day_num = (month-1)*30 +day\n",
    "    prices_smooth = np.array([0,0,0],dtype=float)\n",
    "    for i in range(3):\n",
    "        prices_smooth[i] = (prices[i]*(360-day_num)+prices[i+1]*day_num)/360\n",
    "    \n",
    "    return pd.DataFrame(data=[[prices_smooth[0],prices_smooth[1],prices_smooth[2],close,pe]],columns=[\"p0\",\"p1\",\"p2\",\"close\",\"pe_rollingmedian\"])\n",
    "\n",
    "\n",
    "def single_stock_plotting_v1(sid,conn):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    sid: the stock code, 000001.SZ for example\n",
    "    conn: the sql connection\n",
    "\n",
    "    function: \n",
    "    plot the close_price and the con_est_prices(p0,p1,p2)\n",
    "    \"\"\"\n",
    "    df = get_merged_eps_pe_price(sid,conn)\n",
    "    sid=df[\"sid\"][df.index[0]]\n",
    "    \n",
    "    df=df.groupby(\"DataDate\").apply(get_prices_v1)\n",
    "    df=df.reset_index().set_index(\"DataDate\")\n",
    "    del df[\"level_1\"]\n",
    "    df.index=pd.to_datetime(df.index)\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.plot(df[[\"p0\",\"p1\",\"p2\",\"close\"]])\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title(sid+\" close_price and the con_est_prices\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse the perfromance of the strategy on single stock\n",
    "def get_prices_v2(daily_df,column = \"con_est_eps\"):\n",
    "    close=daily_df[\"S_DQ_ADJCLOSE\"].values[0]\n",
    "    date=daily_df[\"DataDate\"].values[0]\n",
    "\n",
    "    est_eps_arr=np.array([0,0,0,0],dtype=float)\n",
    "    for i in range(4):\n",
    "        try:\n",
    "            reporting_period=str(int(date[0:4])+i-1)+\"1231\"\n",
    "            con_est_eps=daily_df[daily_df[\"REPORTING_PERIOD\"]==reporting_period]\\\n",
    "                        [column].values[0]\n",
    "            est_eps_arr[i]=con_est_eps\n",
    "            if est_eps_arr[i] < 0:\n",
    "                est_eps_arr[i] = np.nan\n",
    "        except IndexError as e:\n",
    "            pass\n",
    "    \n",
    "    if column == \"con_est_eps\":\n",
    "        #若第三年con_eps 为空，则线性填充第三年数据\n",
    "        if est_eps_arr[2]==0:\n",
    "            est_eps_arr[2]= est_eps_arr[1]+(est_eps_arr[1]-est_eps_arr[0])\n",
    "\n",
    "        #处理三年con_eps 不随时间递增的情况 (p0, p1 可能因为一大笔非常规性收益而异常过大)\n",
    "        if est_eps_arr[0]>est_eps_arr[1] and est_eps_arr[1]<est_eps_arr[2]:\n",
    "            est_eps_arr[0] = est_eps_arr[1]-(est_eps_arr[2]-est_eps_arr[1])\n",
    "        elif est_eps_arr[1]>est_eps_arr[2] and est_eps_arr[0]<est_eps_arr[2]:\n",
    "            est_eps_arr[1] = (est_eps_arr[0]+est_eps_arr[2])/2\n",
    "\n",
    "        #若第四年con_eps 为空，则线性填充第四年数据\n",
    "        if est_eps_arr[1]>= est_eps_arr[2]*0.5:\n",
    "            est_eps_arr[3]=est_eps_arr[2]+(est_eps_arr[2]-est_eps_arr[1])\n",
    "    \n",
    "    est_eps_t = est_eps_arr[1]\n",
    "    pb = daily_df[\"S_VAL_PB_NEW\"].iloc[0]\n",
    "    \n",
    "    return pd.DataFrame(data=[[est_eps_arr[0],est_eps_arr[1],est_eps_arr[2],est_eps_arr[3],close,close/est_eps_t,pb]],\\\n",
    "                        columns=[\"est_eps0\",\"est_eps1\",\"est_eps2\",\"est_eps3\",\"close\",\"price/est1\",\"pb\"])\n",
    "    \n",
    "    \n",
    "\n",
    "def single_stock_plotting_v2(sid,conn,consecutive_rolling,use_pe = 0,start_date =\"20160101\",end_date = \"20200310\"):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    sid: the stock code, 000001.SZ for example\n",
    "    conn: the sql connection\n",
    "\n",
    "    function: \n",
    "    plot the close_price and the con_est_prices(p0,p1,p2)\n",
    "    \"\"\"\n",
    "    df = get_merged_eps_pe_price(sid,conn)\n",
    "    sid=df[\"sid\"].iloc[0]\n",
    "    \n",
    "    df = df[df[\"DataDate\"] >= str(int(start_date[0:4])-1)+\"0101\"]\n",
    "    df = df[df[\"DataDate\"] <= end_date]\n",
    "    \n",
    "    df=df.groupby(\"DataDate\").apply(lambda x:get_prices_v2(x))\n",
    "    df=df.reset_index().drop([\"level_1\"],axis = 1)\n",
    "    \n",
    "    #连续 rolling_percentile\n",
    "    if consecutive_rolling == True:\n",
    "        df[\"price/est_bottom\"]  = df[\"price/est1\"].rolling(252).apply(lambda x:np.percentile(x,40))\n",
    "    \n",
    "    #离散 rolling_percentile\n",
    "    else:\n",
    "        df[\"year\"] = df[\"DataDate\"].apply(lambda x:int(x[0:4]))\n",
    "        get_bottom = lambda x:(np.percentile(x.iloc[80:],10) if  len(x)>80 \\\n",
    "                               else np.percentile(x,10) )\n",
    "\n",
    "        get_middle = lambda x: np.percentile(x,40)\n",
    "        df_pe_est_bottom =df[\"price/est1\"].groupby(df[\"year\"]).apply(get_bottom).to_frame(\"price/est_bottom\")\n",
    "        df_pe_est_bottom = df_pe_est_bottom.reset_index()\n",
    "        df_pe_est_bottom[\"year\"] = df_pe_est_bottom[\"year\"].apply(lambda x:x)\n",
    "        df = pd.merge(df,df_pe_est_bottom,on = \"year\",how = \"left\")\n",
    "        \n",
    "        \n",
    "    df = df[df[\"DataDate\"]>=start_date]\n",
    "    \n",
    "    df = df.set_index(\"DataDate\")\n",
    "    df.index=pd.to_datetime(df.index) \n",
    "    \n",
    "    if use_pe == 0:\n",
    "        for i in range(4):\n",
    "            df[\"p\"+str(i)] = df[\"price/est_bottom\"].values*df[\"est_eps\"+str(i)].values\n",
    "    else:\n",
    "        for i in range(4):\n",
    "            df[\"price/est_bottom\"]  =  use_pe\n",
    "            df[\"p\"+str(i)] = use_pe*df[\"est_eps\"+str(i)].values\n",
    "    \n",
    "    \n",
    "    df[\"price/est1\"] = df[\"price/est1\"].apply(lambda x:min(x,200))\n",
    "    df[[\"price/est1\",\"price/est_bottom\"]].plot()\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    industry_plotting.savefig()\n",
    "    plt.show()\n",
    "    \n",
    "    df[[\"p0\",\"p1\",\"p2\",\"p3\",\"close\"]].plot()\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title(sid+\" close_price and the con_est_prices\")\n",
    "    industry_plotting.savefig()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 银行PB定价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_stock_plotting_pb(sid,conn,use_pb,start_date =\"20160101\",end_date = \"20200310\"):\n",
    "    sql = \"\"\"select S_INFO_WINDCODE,EST_DT,EST_REPORT_DT,S_EST_AVGBPS,S_EST_MEDIANBPS from AShareConsensusData \n",
    "    where S_INFO_WINDCODE  = '{}' and  EST_DT between \"20100101\" and \"20200310\" \"\"\".format(sid).upper()\n",
    "    \n",
    "    df = pd.read_sql_query(sql,conn)\n",
    "    df = df.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"EST_DT\":\"DataDate\",\"EST_REPORT_DT\":\"REPORTING_PERIOD\"})\n",
    "    sid=df[\"sid\"].iloc[0]\n",
    "    \n",
    "     #加入收盘价序列\n",
    "    sql = \"\"\"SELECT S_INFO_WINDCODE, TRADE_DT, S_DQ_ADJCLOSE,S_DQ_ADJFACTOR  from ASHAREEODPRICES \n",
    "    where S_INFO_WINDCODE = '{}' and  TRADE_DT between '{}0101' and '20201231'\"\"\".format(sid,2010)    \n",
    "    df_close_hfq = pd.read_sql_query(sql, conn)\n",
    "    df_close_hfq=df_close_hfq.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"TRADE_DT\":\"DataDate\"})\n",
    "    df_close_hfq=df_close_hfq.sort_values([\"sid\",\"DataDate\"])\n",
    "    df_close_hfq[\"S_DQ_ADJCLOSE\"]=df_close_hfq[\"S_DQ_ADJCLOSE\"]/df_close_hfq[\"S_DQ_ADJFACTOR\"].iloc[-1]\n",
    "    df =pd.merge(df,df_close_hfq,on=[\"sid\",\"DataDate\"],how=\"outer\")\n",
    "    \n",
    "    #加入PB序列\n",
    "    sql = \"\"\"SELECT S_INFO_WINDCODE, TRADE_DT,S_VAL_PB_NEW from ASHAREEODDERIVATIVEINDICATOR\n",
    "    where S_INFO_WINDCODE = '{}' and TRADE_DT between '{}0101' and '20201231'\"\"\".format(sid,2010)  \n",
    "    df_PB = pd.read_sql_query(sql, conn)\n",
    "    df_PB = df_PB.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"TRADE_DT\":\"DataDate\"})\n",
    "    df_PB = df_PB.sort_values([\"sid\",\"DataDate\"])\n",
    "    df =pd.merge(df,df_PB,on=[\"sid\",\"DataDate\"],how=\"outer\")\n",
    "    \n",
    "    df = df[df[\"DataDate\"] >= str(int(start_date[0:4])-1)+\"0101\"]\n",
    "    df = df[df[\"DataDate\"] <= end_date]\n",
    "    \n",
    "    df=df.groupby(\"DataDate\").apply(lambda x:get_prices_v2(x,column=\"S_EST_MEDIANBPS\" ))\n",
    "    df=df.reset_index().drop([\"level_1\"],axis = 1)\n",
    "    \n",
    "    df = df.replace(0.0,np.nan)\n",
    "    df = df.sort_values([\"DataDate\"]).fillna(method = \"ffill\")\n",
    "    \n",
    "    df[\"est_eps0\"] = df[\"est_eps1\"] - (df[\"est_eps2\"] - df[\"est_eps1\"])\n",
    "    df[\"est_eps3\"] = df[\"est_eps2\"] + (df[\"est_eps2\"] - df[\"est_eps1\"])\n",
    "    \n",
    "    df[[\"est_eps0\",\"est_eps1\",\"est_eps2\",\"est_eps3\"]] = use_pb * df[[\"est_eps0\",\"est_eps1\",\"est_eps2\",\"est_eps3\"]] \n",
    "    df[[\"est_eps0\",\"est_eps1\",\"est_eps2\",\"est_eps3\"]] = df[[\"est_eps0\",\"est_eps1\",\"est_eps2\",\"est_eps3\"]].rolling(120,axis = 0).apply(np.nanmean)\n",
    "    \n",
    "    df = df.set_index(\"DataDate\")\n",
    "    df.index=pd.to_datetime(df.index) \n",
    "    \n",
    "    df[\"pb_bottom\"] = use_pb\n",
    "    df[[\"pb\",\"pb_bottom\"]].plot()\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    df[[\"est_eps0\",\"est_eps1\",\"est_eps2\",\"est_eps3\",\"close\"]].plot()\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(**DB_INFO, charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "bank_code_ls = [\"601398.SH\",\"601988.SH\",\"601288.SH\",\"601939.SH\",\"601328.SH\"\\\n",
    "               ,\"601166.SH\",\"600016.SH\",\"601998.SH\",\"600015.SH\",\"601818.SH\"]\n",
    "pb_bottom_ls = [0.8,0.65,0.7,0.8,0.6,0.7,0.6,0.65,0.55,0.7]\n",
    "for index,sid in enumerate(bank_code_ls):\n",
    "    print(sid)\n",
    "    df= single_stock_plotting_pb(sid,conn,start_date=\"20140501\",use_pb=pb_bottom_ls[index],end_date=\"20200310\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PE定价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"get_adjusted_con_est_eps(\"000001.SZ\",conn) #获取一致预期数据文件\n",
    "single_stock_plotting_v1(\"000001.SZ\",conn)# 绘图，新版本\n",
    "single_stock_plotting_v2(\"000001.SZ\",conn)# 绘图，旧版本\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(18,19):\n",
    "    conn = pymysql.connect(**DB_INFO, charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "    df_indus = pd.read_excel(r\"/home/ywang/tempory task/result/Top_stocks_each_indus.xlsx\",sheet_name=i)\n",
    "    industry_plotting = PdfPages('result//industry_plotting//industry_{}.pdf'.format(i))\n",
    "    for sid in df_indus[\"sid\"]:\n",
    "        if sid == '600968.SH':\n",
    "            continue\n",
    "        try:\n",
    "            get_adjusted_con_est_eps(sid,conn) #获取一致预期数据文件\n",
    "            single_stock_plotting_v2(sid,conn,consecutive_rolling=False,start_date=\"20151201\",end_date=\"20200310\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    industry_plotting.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(**DB_INFO, charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "get_adjusted_con_est_eps(\"601398.SH\",conn) #获取一致预期数据文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(**DB_INFO, charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "single_stock_plotting_v2(\"601398.SH\",conn,consecutive_rolling=True,use_pe=6,start_date=\"20160501\",end_date=\"20200310\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
