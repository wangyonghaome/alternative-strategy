{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "from data_tools.api import *\n",
    "from utilscht.Data import *\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from joblib import Parallel,delayed\n",
    "import tushare as ts\n",
    "import math\n",
    "import datetime\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "\n",
    "##命令行获得日期参数\n",
    "if 0:\n",
    "    TODAY_DATE=str(sys.argv[1])\n",
    "    NEXT_DATE=str(get_next_trade_date(TODAY_DATE))\n",
    "    month = int(TODAY_DATE[4:6])\n",
    "    if month >= 5:\n",
    "        START_YEAR=int(TODAY_DATE[0:4])-1 #前一年作为基准年\n",
    "    else:\n",
    "        START_YEAR=int(TODAY_DATE[0:4])-2 #前前一年作为基准年\n",
    "#当日日期作为日期参数\n",
    "else:\n",
    "    NEXT_DATE=str(datetime.datetime.now().date())\n",
    "    NEXT_DATE=NEXT_DATE.replace('-','')\n",
    "    TODAY_DATE=str(get_previous_trade_date(NEXT_DATE))\n",
    "    month = int(TODAY_DATE[4:6])\n",
    "    if month >= 5:\n",
    "        START_YEAR=int(TODAY_DATE[0:4])-1 #前一年作为基准年\n",
    "    else:\n",
    "        START_YEAR=int(TODAY_DATE[0:4])-2 #前前一年作为基准年\n",
    "\n",
    "DB_INFO = dict(host='192.168.1.234',\n",
    "               user='winduser',\n",
    "               password='1qaz@WSX',\n",
    "               db='wind')\n",
    "\n",
    "conn = pymysql.connect(**DB_INFO, charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,#控制台打印的日志级别\n",
    "                    filename=r'/home/ywang/proj_con_expect/logging/con_expect_strategy.log',\n",
    "                    filemode='a',##模式，有w和a，w就是写模式，每次都会重新写日志，覆盖之前的日志\n",
    "                    #a是追加模式，默认如果不写的话，就是追加模式\n",
    "                    format='%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s'#日志格式\n",
    "                   )\n",
    "\n",
    "\n",
    "def apply_parallel(df_grouped, func, n_jobs=16, backend='loky', as_index=True, **kwargs):\n",
    "    \"\"\"\n",
    "    This is for parallel between grouped generated by pd.DataFrame.groupby\n",
    "    :param df_grouped:\n",
    "    :param func:\n",
    "    :param n_jobs:\n",
    "    :param backend:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    names = []\n",
    "    groups = []\n",
    "    for name, group in df_grouped:\n",
    "        names.append(name)\n",
    "        groups.append(group)\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=5, backend=backend, batch_size='auto') \\\n",
    "        (delayed(func)(group, **kwargs) for group in groups)\n",
    "\n",
    "    return pd.concat(results, keys=names if as_index else None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###获得所需数据\n",
    "def Get_Required_Data():\n",
    "    #获取行业和市值\n",
    "    indus_size_df=query_table(\"DailyBar\",start_date=\"{}0101\".format(START_YEAR),end_date=TODAY_DATE,fields=[\"L1_INDUSTRY\",\"mktcap\"])\n",
    "    indus_size_df[\"DataDate\"]=indus_size_df[\"DataDate\"].apply(lambda x:str(x))\n",
    "    print(\"finish\")\n",
    "\n",
    "    #获取研报预期数据\n",
    "    sql = \"\"\"SELECT S_INFO_WINDCODE, EST_DT, REPORTING_PERIOD,RESEARCH_INST_NAME, ANALYST_NAME,EST_NET_PROFIT from ASHAREEARNINGEST where EST_DT between '{}0101' and '20201231'\"\"\".format(START_YEAR)   \n",
    "    df_est = pd.read_sql_query(sql, conn)\n",
    "    df_est.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"EST_DT\":\"DataDate\"},inplace=True)\n",
    "    df_est=df_est.sort_values([\"sid\",\"DataDate\",\"REPORTING_PERIOD\"])\n",
    "    df_est=pd.merge(df_est,indus_size_df,on=[\"sid\",\"DataDate\"],how=\"left\")\n",
    "\n",
    "    #获取发布的年报数据\n",
    "    sql = \"\"\"SELECT S_INFO_WINDCODE, ANN_DT, REPORT_PERIOD,S_FA_DEDUCTEDPROFIT,S_FA_EXTRAORDINARY\n",
    "    from ASHAREFINANCIALINDICATOR where ANN_DT between '{}0101' and '20201231'\"\"\".format(START_YEAR)\n",
    "    df_FA_EPS = pd.read_sql_query(sql, conn)\n",
    "    df_FA_EPS.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"REPORT_PERIOD\":\"REPORTING_PERIOD\"},inplace=True)\n",
    "    df_est=pd.merge(df_est,df_FA_EPS,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "\n",
    "    #获取总股本信息\n",
    "    sql=\"select S_INFO_WINDCODE,TOT_SHR,CHANGE_DT from AShareCapitalization\".upper()\n",
    "    df_tot_share=pd.read_sql_query(sql,conn)\n",
    "    df_tot_share.rename(columns={\"S_INFO_WINDCODE\":\"sid\"},inplace=True)\n",
    "    df_tot_share=df_tot_share.sort_values([\"sid\",\"CHANGE_DT\"])\n",
    "    df_now_tot_share=df_tot_share.groupby('sid',as_index=False).apply(lambda x:x.iloc[-1])\n",
    "\n",
    "    #计算前复权每股预期收益\n",
    "    df_est=pd.merge(df_est,df_now_tot_share,on='sid',how='left')\n",
    "    df_est['EST_EPS_DILUTED']=df_est['EST_NET_PROFIT']/df_est['TOT_SHR']\n",
    "    df_est['S_FA_EPS_DILUTED']=(df_est['S_FA_DEDUCTEDPROFIT']+df_est['S_FA_EXTRAORDINARY'])/df_est['TOT_SHR']/10000\n",
    "\n",
    "    #取企业快报数据\n",
    "    sql_1 = \"\"\"SELECT S_INFO_WINDCODE, ANN_DT, REPORT_PERIOD,NET_PROFIT_EXCL_MIN_INT_INC\n",
    "    from ASHAREPROFITEXPRESS where ANN_DT between '{}0101' and '20201231'\"\"\".format(START_YEAR)\n",
    "\n",
    "    df_ProfitExpress = pd.read_sql_query(sql_1, conn)\n",
    "    df_ProfitExpress.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"REPORT_PERIOD\":\"REPORTING_PERIOD\"},inplace=True)\n",
    "    print(\"finish\")\n",
    "\n",
    "    #取企业预报数据\n",
    "    sql_2 = \"\"\"SELECT S_INFO_WINDCODE, S_PROFITNOTICE_DATE, S_PROFITNOTICE_PERIOD,S_PROFITNOTICE_NETPROFITMIN,S_PROFITNOTICE_NETPROFITMAX\n",
    "     from ASHAREPROFITNOTICE where S_PROFITNOTICE_DATE between '{}0101' and '20201231'\"\"\".format(START_YEAR)\n",
    "    df_ProfitNotice = pd.read_sql_query(sql_2, conn)\n",
    "    df_ProfitNotice.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"S_PROFITNOTICE_PERIOD\":\"REPORTING_PERIOD\"},inplace=True)\n",
    "    print(\"finish\")\n",
    "\n",
    "    #获得每天的收盘价序列\n",
    "\n",
    "    df_close=query_table(\"DailyBar\",start_date=\"{}0101\".format(START_YEAR),end_date=TODAY_DATE,fields=[\"close\"])\n",
    "    df_close=df_close.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"TRADE_DT\":\"DataDate\"})\n",
    "    df_close=df_close.sort_values([\"sid\",\"DataDate\"])\n",
    "    df_close[\"DataDate\"]=df_close[\"DataDate\"].apply(str)\n",
    "    #df_close.to_csv(r\"data/close_price.csv\",index=False)\n",
    "    print(\"finish\")\n",
    "\n",
    "\n",
    "    #获得过去两年PE_TTM的 rolling_median 值（相较于mean值更为稳定，不易受极端值影响）\n",
    "\n",
    "    df_PETTM=query_table(\"DailyBar\",start_date=\"{}0101\".format(START_YEAR-1),\\\n",
    "                         end_date=TODAY_DATE,fields=[\"pe_ttm\",\"L1_INDUSTRY\"])\n",
    "    df_PETTM=df_PETTM.rename(columns={\"S_INFO_WINDCODE\":\"sid\",\"TRADE_DT\":\"DataDate\",\"pe_ttm\":\"S_VAL_PE_TTM\"})\n",
    "    df_PETTM=df_PETTM.sort_values([\"sid\",\"DataDate\"])\n",
    "    df_PETTM[\"DataDate\"]=df_PETTM[\"DataDate\"].apply(str)\n",
    "\n",
    "    def adj_by_indus_pe(df):\n",
    "        indus_pe = np.nanmedian(df[\"S_VAL_PE_TTM\"]) \n",
    "        stk_pe= df[\"S_VAL_PE_TTM\"].values\n",
    "        df[\"S_VAL_PE_TTM\"] = np.clip(stk_pe, 0,indus_pe*2)\n",
    "        return df\n",
    "    groups = df_PETTM.groupby([\"L1_INDUSTRY\",\"DataDate\"])\n",
    "    df_PETTM = apply_parallel(groups, adj_by_indus_pe).reset_index(drop=True)\n",
    "    df_PETTM = df_PETTM.sort_values([\"sid\",\"DataDate\"])\n",
    "\n",
    "    def Rolling_Median(df):\n",
    "        df[\"S_VAL_PE_TTM\"]=df[\"S_VAL_PE_TTM\"].fillna(method='ffill')\n",
    "        df[\"pe_2y_rollingmedian\"]=df[\"S_VAL_PE_TTM\"].rolling(252*3).apply(lambda x:np.percentile(x,40))\n",
    "        return df\n",
    "\n",
    "    grouped=df_PETTM.groupby(\"sid\")\n",
    "    df_PETTM_2y_rollingmedian=apply_parallel(grouped,Rolling_Median)\n",
    "    print(\"finish\")\n",
    "\n",
    "    return  df_est,df_now_tot_share,df_FA_EPS,df_ProfitExpress,df_ProfitNotice,df_close,df_PETTM_2y_rollingmedian\n",
    "\n",
    "df_est,df_now_tot_share,df_FA_EPS,df_ProfitExpress,df_ProfitNotice,df_close,df_PETTM_2y_rollingmedian=Get_Required_Data()\n",
    "print(\"finish getting data\\n\\n\")\n",
    "\n",
    "###利用去年一致预期数据和今年实际年报数据获得分析师偏差\n",
    "def Get_Analyst_Bias(df_est):\n",
    "    #计算距离年末的月数（第二年初的预测月数为负值）\n",
    "    df_1y_est=df_est.groupby([\"sid\",\"DataDate\",\"ANALYST_NAME\"],as_index=False).apply(lambda x:x.iloc[0]).reset_index(drop=True)\n",
    "    df_1y_est[\"Date_to_Reporting\"]=df_1y_est[[\"DataDate\",\"REPORTING_PERIOD\"]].apply(lambda x:int(x[1][4:6])-int(x[0][4:6]) if x[0][0:4]==x[1][0:4] else int(x[1][4:6])-int(x[0][4:6])-12,axis=1)\n",
    "\n",
    "    #计算偏差比例\n",
    "    df_1y_est[\"Analyst_Bias\"]=np.abs(df_1y_est[\"S_FA_EPS_DILUTED\"]-df_1y_est[\"EST_EPS_DILUTED\"])/df_1y_est[\"S_FA_EPS_DILUTED\"]\n",
    "\n",
    "    #按照年度将偏差对行业、市值、距离年末月数进行回归取残差\n",
    "    def Regress_by_Year(df):\n",
    "        df=df[pd.notnull(df[\"Analyst_Bias\"])&pd.notnull(df[\"L1_INDUSTRY\"])]\n",
    "        if len(df)==0:\n",
    "            return df\n",
    "        bias=np.array(df[\"Analyst_Bias\"])\n",
    "\n",
    "        indus_dummy=np.array(pd.get_dummies(df[\"L1_INDUSTRY\"]))\n",
    "        mktv=np.array(df[\"mktcap\"])\n",
    "        date_to_reporting=np.array(df[\"Date_to_Reporting\"])\n",
    "\n",
    "        model=sm.OLS(bias,np.column_stack([indus_dummy,mktv,date_to_reporting]))\n",
    "        result=model.fit()\n",
    "        df[\"adj_analyst_bias\"]=result.resid\n",
    "\n",
    "        return df\n",
    "\n",
    "    df_1y_est[\"adj_analyst_bias\"]=np.nan\n",
    "    df_1y_est=df_1y_est.groupby(\"REPORTING_PERIOD\",as_index=False).apply(lambda x:Regress_by_Year(x))\n",
    "    \n",
    "    #按照预测年度、分析师来计算偏差平均值作为下一年度该分析师的可信水平\n",
    "    analyst_avg_bias=df_1y_est.groupby([\"REPORTING_PERIOD\",\"ANALYST_NAME\"],as_index=False).apply(lambda x:np.nan if len(x[\"adj_analyst_bias\"])<5 else np.mean(x[\"adj_analyst_bias\"]))\n",
    "    analyst_avg_bias=analyst_avg_bias.to_frame(name=\"analyst_avg_bias\").reset_index()\n",
    "    analyst_avg_bias[\"next_reporting_period\"]=analyst_avg_bias[\"REPORTING_PERIOD\"].apply(lambda x:str(int(x)+10000))\n",
    "    del analyst_avg_bias[\"REPORTING_PERIOD\"]\n",
    "\n",
    "    df_est[\"ANN_PERIOD\"]=df_est[\"DataDate\"].apply(lambda x:x[0:4]+\"1231\" if x[4:6]>'04' else str(int(x[0:4])-1)+'1231')\n",
    "    df_est=pd.merge(df_est,analyst_avg_bias,left_on=[\"ANALYST_NAME\",\"ANN_PERIOD\"],right_on=[\"ANALYST_NAME\",\"next_reporting_period\"],how=\"left\")\n",
    "    print(\"finish\")\n",
    "\n",
    "    return df_est\n",
    "df_est=Get_Analyst_Bias(df_est)\n",
    "print(\"finish getting bias\\n\\n\")\n",
    "\n",
    "\n",
    "###对过去3个月（过去三个月研报数小于10的个股使用过去6月的数据，对于过去六个月没有研报的个股填充nan值）的数据进行过滤，同一分析师对同一个股的多篇研报只取最后一篇\n",
    "###并对过去三个月的一致预期数据按照时间和分析师偏差进行双重加权（时间越近权重越大，分析师偏差越小权重越大）\n",
    "def Get_Con_Est_Eps(df_est):\n",
    "    date_calendar=pd.date_range(\"{}0101\".format(START_YEAR),TODAY_DATE)\n",
    "    date_calendar=[str(i)[0:4]+str(i)[5:7]+str(i)[8:10] for i in date_calendar]\n",
    "\n",
    "    #按照分析师上一年预测偏差以及研报发布时间进行双重加权\n",
    "    def con_est_calcu(est_data):\n",
    "        if len(est_data) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        \n",
    "        analyst_bias=est_data[\"analyst_avg_bias\"]\n",
    "        max_bias=np.nanmax(analyst_bias)\n",
    "        min_bias=np.nanmin(analyst_bias)\n",
    "        if max_bias==min_bias or (math.isnan(min_bias) or math.isnan(max_bias)):\n",
    "            num = len(analyst_bias)\n",
    "            weight_analyst=np.array([1/num]*num)\n",
    "        else:    \n",
    "            weight_analyst=analyst_bias.apply(lambda x:1 if math.isnan(x) else 3-2*(x-min_bias)/(max_bias-min_bias))\n",
    "        weight_analyst=weight_analyst/np.sum(weight_analyst)\n",
    "\n",
    "        est_data[\"date_to_now\"]=est_data[\"DataDate\"].apply(lambda x:date_calendar.index(TODAY_DATE)-date_calendar.index(x))\n",
    "        weight_date=np.power(0.5**(1/45),np.array(est_data[\"date_to_now\"]))\n",
    "        weight_date=weight_date/np.sum(weight_date)\n",
    "\n",
    "        total_weight=weight_analyst*weight_date\n",
    "        total_weight=np.array(total_weight/np.sum(total_weight))\n",
    "\n",
    "        est_eps=est_data[\"EST_EPS_DILUTED\"].values\n",
    "        median = np.median(est_eps)\n",
    "        MAD =  1.483 * np.median(np.abs(est_eps - median))\n",
    "        est_eps = np.clip(est_eps,median-3*MAD,median+3*MAD)\n",
    "        \n",
    "        if len(est_eps)>=3:\n",
    "            max_index = np.argmax(est_eps)\n",
    "            min_index = np.argmin(est_eps)\n",
    "            est_eps = np.delete(est_eps,[max_index,min_index])\n",
    "            total_weight = np.delete(total_weight,[max_index,min_index])\n",
    "            total_weight = total_weight/np.sum(total_weight)\n",
    "        con_est_eps=np.sum(np.array(est_eps)*total_weight)\n",
    "\n",
    "        return con_est_eps\n",
    "\n",
    "    def Process_Single_StkPeriod(df):\n",
    "\n",
    "        date_range=date_calendar[-90:]#取过去90天数据（包含今天）\n",
    "        est_data=df[df[\"DataDate\"].isin(date_range)]\n",
    "        est_data=est_data[pd.notnull(est_data[\"EST_EPS_DILUTED\"])]\n",
    "        est_data=est_data.groupby(\"ANALYST_NAME\").apply(lambda x:x.iloc[-1])#同一分析师多篇研报只取最后一篇\n",
    "\n",
    "        #如果过去90天研报数小于10，则取过去180天研报\n",
    "        if len(est_data)<10:\n",
    "            date_range=date_calendar[-180:]\n",
    "            est_data=df[df[\"DataDate\"].isin(date_range)]\n",
    "            est_data=est_data[pd.notnull(est_data[\"EST_EPS_DILUTED\"])]\n",
    "            est_data=est_data.groupby(\"ANALYST_NAME\").apply(lambda x:x.iloc[-1])#同一分析师多篇研报只取最后一篇\n",
    "\n",
    "        #如果过去180天没有研报，则返回空的DataFrame\n",
    "        if len(est_data)==0:\n",
    "            return pd.DataFrame(index=[TODAY_DATE],columns=[\"con_est_eps\"],data=[np.nan])\n",
    "\n",
    "        con_est_data=con_est_calcu(est_data)\n",
    "        single_stkperiod_result=pd.DataFrame(index=[TODAY_DATE],columns=[\"con_est_eps\"])\n",
    "        single_stkperiod_result.loc[TODAY_DATE,\"con_est_eps\"]=con_est_data\n",
    "\n",
    "        return single_stkperiod_result\n",
    "\n",
    "    df_est=df_est[df_est[\"REPORTING_PERIOD\"]>=str(START_YEAR)+'1231']\n",
    "    grouped=df_est.groupby([\"sid\",\"REPORTING_PERIOD\"])\n",
    "    con_est_eps_df=apply_parallel(grouped,Process_Single_StkPeriod)\n",
    "    con_est_eps_df=con_est_eps_df.reset_index().\\\n",
    "                    rename(columns={\"level_0\":\"sid\",\"level_1\":\"REPORTING_PERIOD\",\"level_2\":\"DataDate\"})\n",
    "\n",
    "    return con_est_eps_df\n",
    "con_est_eps_df=Get_Con_Est_Eps(df_est)\n",
    "print(\"finish getting con_est_eps\\n\\n\")\n",
    "\n",
    "##如果企业发布了业绩预报或者快报，则直接采取业绩预报或者快报的数据\n",
    "def Replace_with_Notice(df):\n",
    "    reporting_period = df[\"REPORTING_PERIOD\"][df.index[0]]\n",
    "    if reporting_period==str(START_YEAR)+'1231':\n",
    "        sid=df[\"sid\"][df.index[0]]\n",
    "        totshare = df_now_tot_share.set_index('sid').loc[sid,'TOT_SHR']\n",
    "        min_profit=df[\"S_PROFITNOTICE_NETPROFITMIN\"][df.index[0]]\n",
    "        max_profit=df[\"S_PROFITNOTICE_NETPROFITMAX\"][df.index[0]]\n",
    "        notice_eps=(min_profit+max_profit)/2/totshare\n",
    "        df[\"con_est_eps\"]=notice_eps\n",
    "    return df\n",
    "\n",
    "con_est_eps_df_merged=pd.merge(con_est_eps_df,df_ProfitNotice,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "con_est_eps_df_merged=con_est_eps_df_merged.groupby([\"sid\",\"REPORTING_PERIOD\"],as_index=False).apply(Replace_with_Notice)\n",
    "\n",
    "\n",
    "def Replace_with_Express(df):\n",
    "    reporting_period=df[\"REPORTING_PERIOD\"][df.index[0]]\n",
    "    if reporting_period==str(START_YEAR)+'1231':\n",
    "        sid=df[\"sid\"][df.index[0]]\n",
    "        totshare = df_now_tot_share.set_index('sid').loc[sid,'TOT_SHR']\n",
    "        express_eps=df[\"NET_PROFIT_EXCL_MIN_INT_INC\"][df.index[0]]/totshare/10000\n",
    "        df[\"con_est_eps\"]=express_eps\n",
    "    return df\n",
    "\n",
    "con_est_eps_df_merged=pd.merge(con_est_eps_df_merged,df_ProfitExpress,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "con_est_eps_df_merged=con_est_eps_df_merged.groupby([\"sid\",\"REPORTING_PERIOD\"],as_index=False).apply(Replace_with_Express)\n",
    "con_est_eps_df_merged=con_est_eps_df_merged[[\"sid\",\"REPORTING_PERIOD\",\"DataDate\",\"con_est_eps\"]]\n",
    "\n",
    "#在企业发布正式年报之后，使用正式年报数据代替一致预期值\n",
    "df_FA_EPS.reset_index(inplace=True)\n",
    "def Replace_with_FA(df):\n",
    "    reporting_period=df[\"REPORTING_PERIOD\"][df.index[0]]\n",
    "    if reporting_period==str(START_YEAR)+'1231' :\n",
    "        sid=df[\"sid\"][df.index[0]]\n",
    "        ann_date=df[\"ANN_DT\"][df.index[0]]\n",
    "        totshare = df_now_tot_share.set_index('sid').loc[sid,'TOT_SHR']\n",
    "        FA_eps=(df['S_FA_DEDUCTEDPROFIT']+df['S_FA_EXTRAORDINARY'])[df.index[0]]/totshare/10000\n",
    "\n",
    "        df[\"con_est_eps\"]=FA_eps\n",
    "    return df\n",
    "\n",
    "con_est_eps_df_merged=pd.merge(con_est_eps_df_merged,df_FA_EPS,on=[\"sid\",\"REPORTING_PERIOD\"],how=\"left\")\n",
    "con_est_eps_df_merged=con_est_eps_df_merged.groupby([\"sid\",\"REPORTING_PERIOD\"],as_index=False).apply(Replace_with_FA)\n",
    "\n",
    "\n",
    "#加入PE值和每天收盘价\n",
    "con_est_eps_df_merged=con_est_eps_df_merged[[\"sid\",\"REPORTING_PERIOD\",\"DataDate\",\"con_est_eps\"]]\n",
    "con_est_eps_df_merged_renewed=pd.merge(con_est_eps_df_merged,df_PETTM_2y_rollingmedian,on=[\"sid\",\"DataDate\"],how=\"left\")\n",
    "con_est_eps_df_merged_renewed=pd.merge(con_est_eps_df_merged_renewed,df_close,on=[\"sid\",\"DataDate\"],how=\"left\")\n",
    "print(\"finish adjusting con_est_eps\\n\\n\")\n",
    "\n",
    "\n",
    "def get_level(close,prices):\n",
    "    prices = prices.copy()\n",
    "    prices.sort()\n",
    "    \n",
    "    level=0\n",
    "    benchmark=0\n",
    "    for p in prices:\n",
    "        if close<p:\n",
    "            return level+(close-benchmark)/(p-benchmark)\n",
    "        else:\n",
    "            level = level+1\n",
    "            benchmark = p\n",
    "    if (prices[2]-prices[1])/prices[1]>0.1:\n",
    "        return level+(close-benchmark)/(prices[2]-prices[1])\n",
    "    else:\n",
    "        return level+(close-benchmark)/(prices[1]*0.1)\n",
    "\n",
    "\"\"\"\n",
    "def get_level_from_rolling_data(arr):\n",
    "    current_close=arr[251,3]\n",
    "    current_estprices=arr[251,0:3]\n",
    "    \n",
    "    close_series=arr[:,3]\n",
    "    bottom_series=arr[:,0]\n",
    "    middle_series = arr[:,1]\n",
    "    ##params adjusting\n",
    "    if np.sum(close_series<(bottom_series+middle_series)/2)/len(close_series)>0.80:\n",
    "        current_estprices_adj = current_close/current_estprices[1]*current_estprices\n",
    "    else:\n",
    "        current_estprices_adj = current_estprices\n",
    "    return get_level(current_close,current_estprices),get_level(current_close,current_estprices_adj)\n",
    "\n",
    "def get_rolling_data(t_date):\n",
    "    trade_dates = get_n_previous_trade_dates(t_date,252)\n",
    "    df_rolling_data =pd.DataFrame()\n",
    "    for date in trade_dates:\n",
    "        df_daily = pd.read_csv(r\"/share/intern_share/stk_over_level/stock_over_level_{}.csv\".format(date))\n",
    "        df_daily[\"date\"] = date\n",
    "        df_rolling_data = pd.concat([df_rolling_data,df_daily])\n",
    "    return df_rolling_data.sort_values([\"date\",\"sid\"])        \n",
    "\"\"\"\n",
    "def Single_Stock_Over_Level(df):\n",
    "    sid=df[\"sid\"][df.index[0]]\n",
    "    date=df[\"DataDate\"][df.index[0]]\n",
    "\n",
    "    result_df=pd.DataFrame(index=[date],columns=[\"over_level\",\"p1\",\"p2\",\"p3\",\"close_price\",\"pe_rolling_mean\"])\n",
    "\n",
    "    prices=np.array([0,0,0,0],dtype=float)\n",
    "    for i in range(4):\n",
    "        try:\n",
    "            reporting_period=str(int(date[0:4])+i-1)+\"1231\"\n",
    "            con_est_eps=df[df[\"REPORTING_PERIOD\"]==reporting_period][\"con_est_eps\"].values[0]\n",
    "            pe_rolling_median=df[df[\"REPORTING_PERIOD\"]==reporting_period][\"pe_2y_rollingmedian\"].values[0]\n",
    "            prices[i]=con_est_eps*pe_rolling_median\n",
    "        except IndexError as e:\n",
    "            pass\n",
    "    \n",
    "    #填充第三年数据\n",
    "    if prices[2]==0:\n",
    "        prices[2]= prices[1]+(prices[1]-prices[0])\n",
    "    \n",
    "    #处理四年con_eps 非线性增长的情况\n",
    "    if prices[0]>prices[1] and prices[1]<prices[2]:\n",
    "        prices[0] = prices[1]-(prices[2]-prices[1])\n",
    "    elif prices[1]>prices[2] and prices[0]<prices[2]:\n",
    "        prices[1] = (prices[0]+prices[2])/2\n",
    "    \n",
    "    #填充第四年数据\n",
    "    if prices[3] ==0:\n",
    "        prices[3]=prices[2]+prices[2]-prices[1]\n",
    "        \n",
    "    #进行平滑（向未来一年进行滚动）\n",
    "    month , day =( int(date[4:6]), int(date[6:8]) )\n",
    "    day_num = (month-1)*30 +day\n",
    "    prices_smooth = np.array([0,0,0],dtype=float)\n",
    "    for i in range(3):\n",
    "        prices_smooth[i] = (prices[i]*(360-day_num)+prices[i+1]*day_num)/360\n",
    "    \n",
    "    try:\n",
    "        close=df[\"close\"][df.index[0]]\n",
    "        level=get_level(close,prices_smooth)\n",
    "        result_df.loc[date,'over_level']=level\n",
    "    except:\n",
    "        result_df.loc[date,'over_level']=np.nan\n",
    "    \n",
    "    result_df.iloc[0,1:4] = prices_smooth\n",
    "    result_df.iloc[0,4] = close\n",
    "    result_df.iloc[0,5] = df[\"pe_2y_rollingmedian\"].values[0]\n",
    "    return result_df\n",
    "\n",
    "All_stock_over_level=con_est_eps_df_merged_renewed.groupby(\"sid\").apply(Single_Stock_Over_Level)\n",
    "All_stock_over_level=All_stock_over_level.replace(np.inf,np.nan).dropna().reset_index(level=1,drop=True)\n",
    "print(\"finish getting over_level\\n\\n\")\n",
    "All_stock_over_level.to_csv(r\"/share/intern_share/stk_over_level/stock_over_level_{}.csv\".format(TODAY_DATE),index_label=\"sid\")\n",
    "logging.info(\"{} finished\".format(TODAY_DATE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = con_est_eps_df_merged[~pd.notnull(con_est_eps_df_merged[\"con_est_eps\"])]\n",
    "temp_df[\"REPORTING_PERIOD\"].value_counts()\n",
    "len(All_stock_over_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_stock_over_level[All_stock_over_level[\"p1\"]>All_stock_over_level[\"p2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Price_over_Level_daily.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(r\"/share/intern_share/stk_over_level/stock_over_level_{}.csv\"\\\n",
    "                      .format(TODAY_DATE))\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whs300=query_table(\"DailyBar\",start_date=\"20200310\",\n",
    "                      end_date=\"20200310\",fields=[\"w_cs500\"])\n",
    "\n",
    "df_whs300[\"sid\"][df_whs300[\"w_cs500\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_eps_hs300 = temp_df.set_index(\"sid\").loc[df_whs300[\"sid\"][df_whs300[\"w_cs500\"]!=0]]\n",
    "con_eps_hs300[pd.notnull(con_eps_hs300[\"over_level\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 沪深300 覆盖率： 85%左右\n",
    "* 中证500 覆盖率： 66%左右\n",
    "* 全指 覆盖率： 40%左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 每只票文件 --> 每日文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "file_names = os.listdir(r\"result/stk_est_eps/\")\n",
    "file_names.remove('.ipynb_checkpoints')\n",
    "\n",
    "df_ls = []\n",
    "for file_name in file_names:\n",
    "    df_stk = pd.read_csv(\"result//stk_est_eps//\"+file_name)\n",
    "    df_stk[\"sid\"] = file_name[8:17]\n",
    "    df_stk = df_stk.set_index(\"sid\").reset_index()\n",
    "    df_stk = df_stk[df_stk[\"p0\"]>0]\n",
    "    df_ls.append(df_stk)\n",
    "df_summary= pd.concat(df_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary=df_summary[df_summary[\"DataDate\"]>='2017-01-01']\n",
    "\n",
    "df_summary.groupby(\"DataDate\").apply(lambda x:x.sort_values(\"sid\")\\\n",
    "                                     .to_csv(r\"/share/intern_share/stk_over_level/stk_over_level_{}.csv\".format(x[\"DataDate\"].iloc[0]),index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"//share//intern_share//stk_over_level//stk_over_level_2020-01-02.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
